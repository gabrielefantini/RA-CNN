{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6163,"status":"ok","timestamp":1642582390264,"user":{"displayName":"Neppa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOn9i3rzntSDr_XYVuxWhb5KaoSGPja_h2ab1AJw=s64","userId":"00499693764068287127"},"user_tz":-60},"id":"xQjpgylUOBVo","outputId":"f61c7782-9e31-4860-a6ae-9af35d01b8c0"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","from torch.utils.data import Dataset, DataLoader\n","import cv2\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch\n","import pandas as pd\n","from sklearn.preprocessing import MultiLabelBinarizer\n","from sklearn.model_selection import train_test_split\n","import os\n","from re import L\n","import time\n","import torch.optim as optim\n","import torch.backends.cudnn as cudnn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","import matplotlib.pyplot as plt\n","\n","torch.cuda.is_available()"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":246,"status":"ok","timestamp":1642510915763,"user":{"displayName":"Gabriele Fontini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03629683380593234300"},"user_tz":-60},"id":"wkUTrZUXMlpY","outputId":"0be4b81d-e028-4a32-8087-df5e73b9f04a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Wed Jan 19 13:08:54 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 466.47       Driver Version: 466.47       CUDA Version: 11.3     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n","| 25%   31C    P8     6W / 120W |    493MiB /  6144MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|    0   N/A  N/A      1096    C+G   ...me\\Application\\chrome.exe    N/A      |\n","|    0   N/A  N/A      1308    C+G   Insufficient Permissions        N/A      |\n","|    0   N/A  N/A      2076    C+G   ...icrosoft VS Code\\Code.exe    N/A      |\n","|    0   N/A  N/A      5940    C+G   C:\\Windows\\explorer.exe         N/A      |\n","|    0   N/A  N/A      7624    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n","|    0   N/A  N/A      8752    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n","|    0   N/A  N/A      9308    C+G   ...8wekyb3d8bbwe\\Cortana.exe    N/A      |\n","|    0   N/A  N/A      9916    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n","|    0   N/A  N/A     10872    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\n","|    0   N/A  N/A     10948    C+G   ...nputApp\\TextInputHost.exe    N/A      |\n","|    0   N/A  N/A     12920    C+G   ...wekyb3d8bbwe\\Video.UI.exe    N/A      |\n","|    0   N/A  N/A     12956    C+G   ...8wekyb3d8bbwe\\GameBar.exe    N/A      |\n","|    0   N/A  N/A     13280    C+G   ...kyb3d8bbwe\\Calculator.exe    N/A      |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":241,"status":"ok","timestamp":1642582409208,"user":{"displayName":"Neppa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOn9i3rzntSDr_XYVuxWhb5KaoSGPja_h2ab1AJw=s64","userId":"00499693764068287127"},"user_tz":-60},"id":"iBlaa0e8PEEa"},"outputs":[],"source":["# plant_loader.py\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","class PlantDataset(Dataset):\n","        def __init__(self, df, image_dir=f'data\\\\train_images\\\\'):\n","\n","            std = 1. / 255.\n","            means = [109.97 / 255., 127.34 / 255., 123.88 / 255.]\n","\n","            self.image_id = df['image'].values\n","            self.labels = df.iloc[:, 1:].values\n","            self.image_dir = image_dir\n","            self.transform = transforms.Compose([\n","                transforms.ToPILImage(),\n","                transforms.Resize((224,224)),\n","                transforms.ToTensor(),\n","                transforms.Normalize(\n","                    mean=means,\n","                    std=[std]*3)\n","            ])\n","\n","        def __len__(self):\n","            return len(self.labels)\n","\n","        def __getitem__(self, idx):\n","            image_id = self.image_id[idx]\n","            label = torch.tensor(self.labels[idx].astype('int8'), dtype=torch.float32)\n","            \n","            image_path = self.image_dir + image_id\n","            image = cv2.imread(image_path)\n","            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","            \n","            image = self.transform(image)\n","\n","            return image, label\n","\n","        @staticmethod\n","        def tensor_to_img(x, imtype=np.uint8):\n","            mean = [109.97 / 255., 127.34 / 255., 123.88 / 255.]\n","            std = [1. / 255., 1. / 255., 1. / 255.]\n","\n","            if not isinstance(x, np.ndarray):\n","                if isinstance(x, torch.Tensor):  # get the data from a variable\n","                    image_tensor = x.data\n","                else:\n","                    return x\n","                image_numpy = image_tensor.cpu().float().numpy()  # convert it into a numpy array\n","                if image_numpy.shape[0] == 1:  # grayscale to RGB\n","                    image_numpy = np.tile(image_numpy, (3, 1, 1))\n","                for i in range(len(mean)):\n","                    image_numpy[i] = image_numpy[i] * std[i] + mean[i]\n","                image_numpy = image_numpy * 255\n","                image_numpy = np.transpose(image_numpy, (1, 2, 0))  # post-processing: tranpose and scaling\n","            else:  # if it is a numpy array, do nothing\n","                image_numpy = x\n","            return image_numpy.astype(imtype)\n","\n","def get_plant_loader():\n","\n","    #read data from csv\n","    df_train = pd.read_csv(f'data/train.csv')\n","\n","    #label distribution\n","    train_count = df_train['labels'].value_counts()\n","\n","    #plot original distribution\n","    #plt.figure(figsize=(20,12))\n","    #labels = sns.barplot(df_train.labels.value_counts().index,df_train.labels.value_counts())\n","    #for item in labels.get_xticklabels():\n","    #    item.set_rotation(45)\n","    \n","    #split labels\n","    df_train['labels'] = df_train['labels'].apply(lambda string: string.split(' '))\n","    #print(df_train.head(n=12))\n","\n","    #make label in binary form\n","    train_df_list = list(df_train['labels'])\n","    mlb = MultiLabelBinarizer()\n","    trainx = pd.DataFrame(mlb.fit_transform(train_df_list), columns=mlb.classes_, index=df_train.index)\n","    \n","    #plot distribution after split\n","    #labels = list(trainx.sum().keys())\n","    #label_counts = trainx.sum().values.tolist()\n","    #fig, ax = plt.subplots(1,1, figsize=(20,6))\n","    #sns.barplot(x= labels, y= label_counts, ax=ax)\n","    \n","    #plot labels distribution\n","    #fig, ax = plt.subplots(figsize=(10,6))\n","    #trainx.sum().plot.bar(title='Target Class Distribution',width=0.2)\n","    #fig.savefig('plots/labels.png')\n","\n","    #concat label in binary form with image name\n","    train_data = pd.concat([df_train, trainx], axis=1).drop('labels', axis=1)\n","\n","    train, validation = train_test_split(train_data, train_size=0.9, random_state=11)    \n","    #test, validation = train_test_split(remaining, test_size=0.5)\n","    #print(train_data)\n","\n","    #print(len(train), len(validation))\n","\n","    return {\n","        \"train\": PlantDataset(train),\n","        \"validation\": PlantDataset(validation) \n","        }\n","if __name__ == \"__main__\":\n","    path = 'plots'\n","    if not os.path.exists(path):\n","      os.makedirs(path)\n","    #get_plant_loader()"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"6CWFcZOPPQVD"},"outputs":[],"source":["# pretrain_efficientNet.py\n","\n","def log(msg):\n","    open('build/core.log', 'a').write(f'[{time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())}]\\t'+msg+'\\n'), print(msg)\n","\n","def train(net, dataloader, optimizer, criterion, epoch):\n","    losses = 0\n","    losses20 = 0\n","\n","    for step, (inputs, labels) in enumerate(dataloader, 0):\n","        inputs, labels = Variable(inputs).cuda(), Variable(labels).cuda()\n","        optimizer.zero_grad()\n","        outputs = net(inputs)\n","\n","        outputs = torch.sigmoid(outputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        losses += loss.item()\n","        losses20 += loss.item()\n","\n","        if step % 20 == 0 and step != 0:\n","           avg_loss20 = losses20/20\n","           log(f':: loss @step({step:2d}/{len(dataloader)})-epoch{epoch}: {loss:.10f}\\tavg_loss_20: {avg_loss20:.10f}')\n","           losses20 = 0\n","\n","    avg_loss = losses/len(dataloader)\n","    #log(f':: loss @epoch{epoch}: avg_loss: {avg_loss:.10f}')\n","    return avg_loss\n","\n","def eval(net, dataloader, epoch, interval):\n","    #log(' :: Testing on validation set ...')\n","    correct_top1 = 0\n","    #correct_top3 = 0\n","    #correct_top5 = 0\n","\n","    accuracies = []\n","    \n","    for step, (inputs, labels) in enumerate(dataloader, 0):\n","        inputs, labels = Variable(inputs).cuda(), Variable(labels).cuda()\n","\n","        with torch.no_grad():\n","            logits = net(inputs)\n","            logits = torch.sigmoid(logits)\n","            logits[logits >= 0.5 ] = 1\n","            logits[logits < 0.5 ] = 0\n","            #print(\"logits:\", logits, \"labels:\", labels)\n","\n","            #accuracy = torch.all(torch.eq(logits, labels),  dim=1).sum()/len(labels)\n","            #print(accuracy)\n","            \n","            #accuracies += torch.all(torch.eq(logits, labels),  dim=1).sum()\n","            correct_top1 += torch.all(torch.eq(logits, labels),  dim=1).sum()\n","            current_accuracy = correct_top1/((step+1)*int(inputs.shape[0]))\n","            accuracies.append(current_accuracy.item())\n","\n","            #print(\"step: \", step)\n","            #print(correct_top1/((step+1)*int(inputs.shape[0])))\n","            #correct_top1 += torch.eq(logits.topk(max((1, 1)), 1, True, True)[1], labels.view(-1, 1)).sum().float().item()\n","            #correct_top3 += torch.eq(logits.topk(max((1, 3)), 1, True, True)[1], labels.view(-1, 1)).sum().float().item()\n","            #correct_top5 += torch.eq(logits.topk(max((1, 5)), 1, True, True)[1], labels.view(-1, 1)).sum().float().item()\n","\n","            if step % interval == 0 and step != 0:\n","               print(f':: accuracy @step({step:2d}/{len(dataloader)})-epoch{epoch}: {current_accuracy:.5%}')\n","\n","        #if step == 57:\n","            #log(f'\\tAccuracy@top1 ({step}/{len(dataloader)}) = {correct_top1/((step+1)*int(inputs.shape[0])):.5%}')\n","            #log(f'\\tAccuracy@top3 ({step}/{len(dataloader)}) = {correct_top3/((step+1)*int(inputs.shape[0])):.5%}')\n","            #log(f'\\tAccuracy@top5 ({step}/{len(dataloader)}) = {correct_top5/((step+1)*int(inputs.shape[0])):.5%}')\n","            #return correct_top1/((step+1)*int(inputs.shape[0]))\n","    tmp = np.array(accuracies)\n","    avg_accuracy = tmp.mean()\n","    return avg_accuracy \n","\n","def run():\n","    state_dict = torchvision.models.efficientnet_b0(pretrained=True).state_dict()\n","    state_dict.pop('classifier.1.weight')\n","    state_dict.pop('classifier.1.bias')\n","    net = torchvision.models.efficientnet_b0(num_classes=6).cuda()\n","\n","    state_dict['classifier.1.weight'] = net.state_dict()['classifier.1.weight']\n","    state_dict['classifier.1.bias'] = net.state_dict()['classifier.1.bias']\n","    net.load_state_dict(state_dict)\n","    cudnn.benchmark = True\n","\n","   \n","    #for param in net.parameters():\n","    #    print(type(param), param.size())\n","\n","    criterion = torch.nn.BCELoss()\n","    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n","\n","    data_set = get_plant_loader()\n","\n","    trainloader = DataLoader(data_set[\"train\"], batch_size=32, shuffle=True)\n","    validationloader = DataLoader(data_set[\"validation\"], batch_size=32, shuffle=False)\n","    \n","    losses = []\n","    validation_accuracies = []\n","    training_accuracies = []\n","\n","    # 15 epochs\n","    # 7 epochs = 77.48% cca\n","    for epoch in range(7):  # loop over the dataset multiple times\n","            accuracy = 0\n","\n","            log(' :: Training on training set ...')\n","            avg_loss = train(net,trainloader,optimizer,criterion,epoch)\n","            log(f':: loss @epoch{epoch}: avg_loss: {avg_loss:.10f}')\n","\n","            log(' :: Testing on validation set ...')\n","            temp_accuracy = eval(net, validationloader, epoch, 5)\n","            log(f':: validation set accuracy @epoch{epoch}: avg_accuracy: {temp_accuracy:.5%}')\n","\n","            if temp_accuracy > accuracy :\n","                accuracy = temp_accuracy\n","                #stamp = f'e{epoch}{int(time.time())}'\n","                torch.save(net, f'build/efficientNet_b0_ImageNet.pt')\n","                torch.save(optimizer.state_dict, f'build/optimizer.pt')\n","\n","            log(' :: Testing on training set ...')\n","            train_accuracy = eval(net, trainloader, epoch, 20)\n","            log(f':: training set accuracy @epoch{epoch}: avg_accuracy: {train_accuracy:.5%}')\n","\n","            losses.append(avg_loss)\n","            validation_accuracies.append(temp_accuracy)\n","            training_accuracies.append(train_accuracy)\n","\n","    np.savetxt(f'logs/efficientNet_b0-loss.csv',losses,'%.10f',',')\n","    np.savetxt(f'logs/efficientNet_b0-validation_accuracy.csv',validation_accuracies,'%.10f',',')\n","    np.savetxt(f'logs/efficientNet_b0-training_accuracy.csv',training_accuracies,'%.10f',',')\n","    \n","\n","if __name__ == \"__main__\":\n","    path = 'build'\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","    path = 'logs'\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","    #run()\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":1068,"status":"ok","timestamp":1642582421336,"user":{"displayName":"Neppa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOn9i3rzntSDr_XYVuxWhb5KaoSGPja_h2ab1AJw=s64","userId":"00499693764068287127"},"user_tz":-60},"id":"rF-OA2ADM_Lu"},"outputs":[],"source":["# model.py\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","import torchvision\n","\n","\n","class AttentionCropFunction(torch.autograd.Function):\n","    @staticmethod\n","    def forward(self, images, locs):\n","        \n","        def h(_x): return 1 / (1 + torch.exp(-10 * _x.float()))\n","\n","        #in_size = 224\n","        in_size = images.size()[2]\n","        #unit = tensor([[  0,   1,   2,  ..., 221, 222, 223],\n","        #               [  0,   1,   2,  ..., 221, 222, 223],\n","        #               [  0,   1,   2,  ..., 221, 222, 223],\n","        #               ...,\n","        #               [  0,   1,   2,  ..., 221, 222, 223],\n","        #               [  0,   1,   2,  ..., 221, 222, 223],\n","        #               [  0,   1,   2,  ..., 221, 222, 223]])\n","        unit = torch.stack([torch.arange(0, in_size)] * in_size)\n","        #x è 3 tensor([[[  0,   0,   0,  ...,   0,   0,   0],\n","        #               [  1,   1,   1,  ...,   1,   1,   1],\n","        #               [  2,   2,   2,  ...,   2,   2,   2],\n","        #               ...,\n","        #               [221, 221, 221,  ..., 221, 221, 221],\n","        #               [222, 222, 222,  ..., 222, 222, 222],\n","        #               [223, 223, 223,  ..., 223, 223, 223]]\n","        x = torch.stack([unit.t()] * 3)\n","        #y è 3 tensori unit\n","        y = torch.stack([unit] * 3)\n","        if isinstance(images, torch.cuda.FloatTensor):\n","            x, y = x.cuda(), y.cuda()\n","\n","        in_size = images.size()[2]\n","        ret = []\n","\n","        #processa tutte le immagini del batch, in questo caso images.size(0) è 12\n","        for i in range(images.size(0)):\n","            # locs sono tensor([[143.3381, 124.7122,  51.1188],\n","            #                   [110.5597, 137.9310,  60.7936],\n","            #                   [119.1002,  99.9731,  52.5289],\n","            #                   [113.5314, 129.4575,  46.4848],\n","            #                   [ 79.8117,  94.2794,  54.7804],\n","            #                   [117.5080, 136.6172,  63.8196],\n","            #                   [ 87.7556, 113.1122,  62.1823],\n","            #                   [131.8735, 108.5016,  63.9460],\n","            #                   [ 99.1213, 125.6766,  76.2739],\n","            #                   [ 90.7646,  89.5686,  49.1010],\n","            #                   [ 84.4682, 130.7239,  82.1634],\n","            #                   [ 88.4385,  96.4715,  42.9040]], device='cuda:0',\n","            #               grad_fn=<MulBackward0>)\n","\n","            tx, ty, tl = locs[i][0], locs[i][1], locs[i][2]\n","            \n","            #per evitare che si rimpicciolisca troppo\n","            tl = tl if tl > (in_size/3) else in_size/3\n","            #check per evitare che il crop \"sbordi\"\n","            tx = tx if tx > tl else tl\n","            tx = tx if tx < in_size-tl else in_size-tl\n","            ty = ty if ty > tl else tl\n","            ty = ty if ty < in_size-tl else in_size-tl\n","            \n","            # in un esempio, w_off, h_off, w_end, h_end 68 50 218 199\n","            w_off = int(tx-tl) if (tx-tl) > 0 else 0\n","            h_off = int(ty-tl) if (ty-tl) > 0 else 0\n","            w_end = int(tx+tl) if (tx+tl) < in_size else in_size\n","            h_end = int(ty+tl) if (ty+tl) < in_size else in_size\n","            \n","            #mk cosa fa di preciso?????\n","            mk = (h(x-w_off) - h(x-w_end)) * (h(y-h_off) - h(y-h_end))\n","            xatt = images[i] * mk\n","            xatt_cropped = xatt[:, w_off: w_end, h_off: h_end]\n","            before_upsample = Variable(xatt_cropped.unsqueeze(0))\n","            #fa l'upsampling dell'immagine croppata a 224x224\n","            xamp = F.upsample(before_upsample, size=(224, 224), mode='bilinear', align_corners=True)\n","            ret.append(xamp.data.squeeze())\n","\n","        ret_tensor = torch.stack(ret)\n","        self.save_for_backward(images, ret_tensor)\n","        return ret_tensor\n","\n","    @staticmethod\n","    def backward(self, grad_output):\n","        images, ret_tensor = self.saved_variables[0], self.saved_variables[1]\n","        in_size = 224\n","        ret = torch.Tensor(grad_output.size(0), 3).zero_()\n","        norm = -(grad_output * grad_output).sum(dim=1)\n","        x = torch.stack([torch.arange(0, in_size)] * in_size).t()\n","        y = x.t()\n","        long_size = (in_size/3*2)\n","        short_size = (in_size/3)\n","        mx = (x >= long_size).float() - (x < short_size).float()\n","        my = (y >= long_size).float() - (y < short_size).float()\n","        ml = (((x < short_size)+(x >= long_size)+(y < short_size)+(y >= long_size)) > 0).float()*2 - 1\n","\n","        mx_batch = torch.stack([mx.float()] * grad_output.size(0))\n","        my_batch = torch.stack([my.float()] * grad_output.size(0))\n","        ml_batch = torch.stack([ml.float()] * grad_output.size(0))\n","\n","        if isinstance(grad_output, torch.cuda.FloatTensor):\n","            mx_batch = mx_batch.cuda()\n","            my_batch = my_batch.cuda()\n","            ml_batch = ml_batch.cuda()\n","            ret = ret.cuda()\n","\n","        ret[:, 0] = (norm * mx_batch).sum(dim=1).sum(dim=1)\n","        ret[:, 1] = (norm * my_batch).sum(dim=1).sum(dim=1)\n","        ret[:, 2] = (norm * ml_batch).sum(dim=1).sum(dim=1)\n","        return None, ret\n","\n","\n","class AttentionCropLayer(nn.Module):\n","    \"\"\"\n","        Crop function sholud be implemented with the nn.Function.\n","        Detailed description is in 'Attention localization and amplification' part.\n","        Forward function will not changed. backward function will not opearate with autograd, but munually implemented function\n","    \"\"\"\n","\n","    def forward(self, images, locs):\n","        return AttentionCropFunction.apply(images, locs)\n","\n","\n","class RACNN(nn.Module):\n","    def __init__(self, num_classes, img_scale=448):\n","        super(RACNN, self).__init__()\n","\n","        self.b1 = torchvision.models.efficientnet_b0(num_classes=num_classes)\n","        self.b2 = torchvision.models.efficientnet_b0(num_classes=num_classes)\n","        self.b3 = torchvision.models.efficientnet_b0(num_classes=num_classes)\n","\n","        self.classifier1 = nn.Linear(320, num_classes)\n","        self.classifier2 = nn.Linear(320, num_classes)\n","        self.classifier3 = nn.Linear(320, num_classes)\n","\n","        self.feature_pool = torch.nn.AdaptiveAvgPool2d(output_size=1)\n","        #self.atten_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.crop_resize = AttentionCropLayer()\n","\n","        #l'output delle due apn sono 3 valori, che indicano x,y,l\n","        self.apn1 = nn.Sequential(\n","            nn.Linear(320 * 7 * 7, 4096),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(4096, 3),\n","            nn.Tanh(),\n","        )\n","\n","        self.apn2 = nn.Sequential(\n","            nn.Linear(320 * 7 * 7, 4096),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(4096, 3),\n","            nn.Tanh(),\n","        )\n","        \n","        self.echo = None\n","\n","    def forward(self, x):\n","        #batch_size = x.shape[0]\n","        rescale_tl = torch.tensor([1, 1, 0.5], requires_grad=False).cuda()\n","        # forward @scale-1\n","        feature_s1 = self.b1.features[:-1](x)  # torch.Size([batch_size, 320, 7, 7])\n","        pool_s1 = self.feature_pool(feature_s1) # torch.Size([batch_size, 320, 1, 1])\n","        _attention_s1 = self.apn1(feature_s1.view(-1, 320 * 7 * 7))\n","        attention_s1 = _attention_s1*rescale_tl\n","        resized_s1 = self.crop_resize(x, attention_s1 * x.shape[-1])\n","        # forward @scale-2\n","        feature_s2 = self.b2.features[:-1](resized_s1)  # torch.Size([1, 320, 7, 7])\n","        pool_s2 = self.feature_pool(feature_s2)\n","        _attention_s2 = self.apn2(feature_s2.view(-1, 320 * 7 * 7))\n","        attention_s2 = _attention_s2*rescale_tl\n","        resized_s2 = self.crop_resize(resized_s1, attention_s2 * resized_s1.shape[-1])\n","        # forward @scale-3\n","        feature_s3 = self.b3.features[:-1](resized_s2)\n","        pool_s3 = self.feature_pool(feature_s3)\n","        pred1 = self.classifier1(pool_s1.view(-1, 320))\n","        pred2 = self.classifier2(pool_s2.view(-1, 320))\n","        pred3 = self.classifier3(pool_s3.view(-1, 320))\n","        return [pred1, pred2, pred3], [feature_s1, feature_s2], [attention_s1, attention_s2], [resized_s1, resized_s2]\n","\n","    def __get_weak_loc(self, features):\n","        ret = []   # search regions with the highest response value in conv5\n","        for i in range(len(features)):\n","            resize = 224 if i >= 1 else 448\n","            response_map_batch = F.interpolate(features[i], size=[resize, resize], mode=\"bilinear\").mean(1)  # mean alone channels\n","            ret_batch = []\n","            for response_map in response_map_batch:\n","                argmax_idx = response_map.argmax()\n","                ty = (argmax_idx % resize)\n","                argmax_idx = (argmax_idx - ty)/resize\n","                tx = (argmax_idx % resize)\n","                ret_batch.append([(tx*1.0/resize).clamp(min=0.25, max=0.75), (ty*1.0/resize).clamp(min=0.25, max=0.75), 0.25])  # tl = 0.25, fixed\n","            ret.append(torch.Tensor(ret_batch))\n","        return ret\n","\n","    def __echo_pretrain_apn(self, inputs, optimizer):\n","        inputs = Variable(inputs).cuda()\n","        _, features, attens, _ = self.forward(inputs)\n","        weak_loc = self.__get_weak_loc(features)\n","        optimizer.zero_grad()\n","        weak_loss1 = F.smooth_l1_loss(attens[0], weak_loc[0].cuda())\n","        weak_loss2 = F.smooth_l1_loss(attens[1], weak_loc[1].cuda())\n","        loss = weak_loss1 + weak_loss2\n","        #calcola il gradiente della loss\n","        loss.backward()\n","        #perform a single optimization step\n","        optimizer.step()\n","        #ritorna la loss come un singolo numero anziche un tensore\n","        return loss.item()\n","\n","    @staticmethod\n","    def multitask_loss(logits, targets):\n","        loss = []\n","        criterion = torch.nn.BCEWithLogitsLoss()\n","        criterion = criterion\n","        for i in range(len(logits)):\n","            loss.append(criterion(logits[i], targets))\n","        loss = torch.sum(torch.stack(loss))\n","        return loss\n","\n","    @staticmethod\n","    def rank_loss(logits, targets, margin=0.05):\n","        #as said in the paper\n","        preds = [torch.sigmoid(x) for x in logits] # preds length equal to 3\n","        losses = []\n","        criterion = torch.nn.MarginRankingLoss(margin=0.05)\n","        for pred in preds:\n","            loss = []\n","            for i in range(len(pred)-1):\n","                #the loss is the diff between cnn predictions\n","                #rank_loss = (pred[i]-pred[i+1] + margin).clamp(min = 0)\n","                y = Variable(torch.Tensor(pred[0].size(0)).fill_(-1)).cuda()\n","                rank_loss = criterion(pred[i], pred[i+1], y)\n","                loss.append(rank_loss)\n","            loss = torch.sum(torch.stack(loss))\n","            losses.append(loss)\n","        losses = torch.stack(losses)\n","        losses = torch.sum(losses)\n","        return losses\n","    \n","    def __echo_backbone(self, inputs, targets, optimizer):\n","        inputs, targets = Variable(inputs).cuda(), Variable(targets).cuda()\n","        logits, _, _, _ = self.forward(inputs)\n","        optimizer.zero_grad()\n","        # logit --> the vector of raw (non-normalized) predictions that a classification model generates\n","        loss = self.multitask_loss(logits, targets)\n","        loss.backward()\n","        optimizer.step()\n","        return loss.item()\n","\n","    def __echo_apn(self, inputs, targets, optimizer):\n","        inputs, targets = Variable(inputs).cuda(), Variable(targets).cuda()\n","        logits, _, _, _ = self.forward(inputs)\n","        optimizer.zero_grad()\n","        loss = self.rank_loss(logits, targets)\n","        loss.backward()\n","        optimizer.step()\n","        return loss.item()\n","\n","    def mode(self, mode_type):\n","        assert mode_type in ['pretrain_apn', 'apn', 'backbone']\n","        if mode_type == 'pretrain_apn':\n","            self.echo = self.__echo_pretrain_apn\n","            self.eval()\n","        if mode_type == 'backbone':\n","            self.echo = self.__echo_backbone\n","            self.train()\n","        if mode_type == 'apn':\n","            self.echo = self.__echo_apn\n","            self.eval()\n","\n","\n","#if __name__ == \"__main__\":\n","#    net = RACNN(num_classes=6).cuda()\n","#    net.mode('pretrain_apn')\n","#    optimizer = torch.optim.SGD(list(net.apn1.parameters()) + list(net.apn2.parameters()), lr=0.001, momentum=0.9)\n","#    for i in range(50):\n","#        inputs = torch.rand(2, 3, 448, 448)\n","#        print(f':: loss @step{i} : {net.echo(inputs, optimizer)}')\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":118377,"status":"ok","timestamp":1642582555992,"user":{"displayName":"Neppa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOn9i3rzntSDr_XYVuxWhb5KaoSGPja_h2ab1AJw=s64","userId":"00499693764068287127"},"user_tz":-60},"id":"G-GYhrJaNewS","outputId":"5a259708-b93d-4046-93f1-28fb7510658e"},"outputs":[{"name":"stdout","output_type":"stream","text":[" :: Cleaning cache dir ...\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\gabri\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\functional.py:3509: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n","  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n","C:\\Users\\gabri\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\functional.py:3631: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  warnings.warn(\n","C:\\Users\\gabri\\AppData\\Local\\Temp/ipykernel_8052/3993585667.py:89: DeprecationWarning: 'saved_variables' is deprecated; use 'saved_tensors'\n","  images, ret_tensor = self.saved_variables[0], self.saved_variables[1]\n"]},{"name":"stdout","output_type":"stream","text":[":: loss @step 0: 0.5570628643035889\tavg_loss_5: 0.5570628643035889\n","0\n",":: loss @step 1: 0.20652124285697937\tavg_loss_5: 0.3817920535802841\n","1\n",":: loss @step 2: 0.09563125669956207\tavg_loss_5: 0.2864051212867101\n","2\n",":: loss @step 3: 0.12864725291728973\tavg_loss_5: 0.246965654194355\n","3\n",":: loss @step 4: 0.14423291385173798\tavg_loss_5: 0.2264191061258316\n","4\n",":: loss @step 5: 0.15619944036006927\tavg_loss_5: 0.14624642133712767\n","5\n",":: loss @step 6: 0.1331322193145752\tavg_loss_5: 0.13156861662864686\n","6\n",":: loss @step 7: 0.16473954916000366\tavg_loss_5: 0.14539027512073516\n","7\n",":: loss @step 8: 0.14791493117809296\tavg_loss_5: 0.1492438107728958\n","8\n",":: loss @step 9: 0.13092544674873352\tavg_loss_5: 0.14658231735229493\n","9\n",":: loss @step10: 0.14041820168495178\tavg_loss_5: 0.14342606961727142\n","10\n",":: loss @step11: 0.1303066611289978\tavg_loss_5: 0.14286095798015594\n","11\n",":: loss @step12: 0.1322145313024521\tavg_loss_5: 0.13635595440864562\n","12\n",":: loss @step13: 0.09797797352075577\tavg_loss_5: 0.12636856287717818\n","13\n",":: loss @step14: 0.21625447273254395\tavg_loss_5: 0.14343436807394028\n","14\n",":: loss @step15: 0.21719953417778015\tavg_loss_5: 0.15879063457250595\n","15\n",":: loss @step16: 0.14388522505760193\tavg_loss_5: 0.16150634735822678\n","16\n",":: loss @step17: 0.13718445599079132\tavg_loss_5: 0.1625003322958946\n","17\n",":: loss @step18: 0.13676241040229797\tavg_loss_5: 0.17025721967220306\n","18\n",":: loss @step19: 0.23228874802589417\tavg_loss_5: 0.1734640747308731\n","19\n",":: loss @step20: 0.15878352522850037\tavg_loss_5: 0.16178087294101715\n","20\n",":: loss @step21: 0.16872268915176392\tavg_loss_5: 0.16674836575984955\n","21\n",":: loss @step22: 0.1305772066116333\tavg_loss_5: 0.16542691588401795\n","22\n",":: loss @step23: 0.24899211525917053\tavg_loss_5: 0.18787285685539246\n","23\n",":: loss @step24: 0.13450706005096436\tavg_loss_5: 0.1683165192604065\n","24\n",":: loss @step25: 0.13258415460586548\tavg_loss_5: 0.16307664513587952\n","25\n",":: loss @step26: 0.261469304561615\tavg_loss_5: 0.18162596821784974\n","26\n",":: loss @step27: 0.2155941128730774\tavg_loss_5: 0.19862934947013855\n","27\n",":: loss @step28: 0.20446020364761353\tavg_loss_5: 0.18972296714782716\n","28\n",":: loss @step29: 0.20056381821632385\tavg_loss_5: 0.20293431878089904\n","29\n",":: loss @step30: 0.15650278329849243\tavg_loss_5: 0.20771804451942444\n","30\n",":: loss @step31: 0.11921530961990356\tavg_loss_5: 0.17926724553108214\n","31\n",":: loss @step32: 0.22631534934043884\tavg_loss_5: 0.18141149282455443\n","32\n",":: loss @step33: 0.21078717708587646\tavg_loss_5: 0.18267688751220704\n","33\n",":: loss @step34: 0.20368950068950653\tavg_loss_5: 0.18330202400684356\n","34\n",":: loss @step35: 0.1938956379890442\tavg_loss_5: 0.19078059494495392\n","35\n",":: loss @step36: 0.1732974797487259\tavg_loss_5: 0.20159702897071838\n","36\n",":: loss @step37: 0.17336896061897278\tavg_loss_5: 0.19100775122642516\n","37\n",":: loss @step38: 0.18819789588451385\tavg_loss_5: 0.18648989498615265\n","38\n",":: loss @step39: 0.13448680937290192\tavg_loss_5: 0.17264935672283171\n","39\n",":: loss @step40: 0.16763168573379517\tavg_loss_5: 0.16739656627178193\n","40\n",":: loss @step41: 0.11542890220880508\tavg_loss_5: 0.15582285076379776\n","41\n",":: loss @step42: 0.1538761854171753\tavg_loss_5: 0.15192429572343827\n","42\n",":: loss @step43: 0.19321183860301971\tavg_loss_5: 0.15292708426713944\n","43\n",":: loss @step44: 0.23404845595359802\tavg_loss_5: 0.17283941358327864\n","44\n",":: loss @step45: 0.1507243514060974\tavg_loss_5: 0.16945794671773912\n","45\n",":: loss @step46: 0.2048182487487793\tavg_loss_5: 0.18733581602573396\n","46\n",":: loss @step47: 0.1780126541852951\tavg_loss_5: 0.1921631097793579\n","47\n",":: loss @step48: 0.12431475520133972\tavg_loss_5: 0.1783836930990219\n","48\n",":: loss @step49: 0.19371257722377777\tavg_loss_5: 0.17031651735305786\n","49\n",":: loss @step50: 0.18035608530044556\tavg_loss_5: 0.1762428641319275\n","50\n",":: loss @step51: 0.21126237511634827\tavg_loss_5: 0.17753168940544128\n","51\n",":: loss @step52: 0.17664745450019836\tavg_loss_5: 0.17725864946842193\n","52\n",":: loss @step53: 0.2414974421262741\tavg_loss_5: 0.2006951868534088\n","53\n",":: loss @step54: 0.2600168287754059\tavg_loss_5: 0.21395603716373443\n","54\n",":: loss @step55: 0.14534156024456024\tavg_loss_5: 0.20695313215255737\n","55\n",":: loss @step56: 0.18506291508674622\tavg_loss_5: 0.20171324014663697\n","56\n",":: loss @step57: 0.1646554172039032\tavg_loss_5: 0.19931483268737793\n","57\n",":: loss @step58: 0.2257823795080185\tavg_loss_5: 0.1961718201637268\n","58\n",":: loss @step59: 0.17457078397274017\tavg_loss_5: 0.17908261120319366\n","59\n",":: loss @step60: 0.19676126539707184\tavg_loss_5: 0.18936655223369597\n","60\n",":: loss @step61: 0.19110839068889618\tavg_loss_5: 0.19057564735412597\n","61\n",":: loss @step62: 0.1982704997062683\tavg_loss_5: 0.197298663854599\n","62\n",":: loss @step63: 0.1833939105272293\tavg_loss_5: 0.18882097005844117\n","63\n",":: loss @step64: 0.15934571623802185\tavg_loss_5: 0.1857759565114975\n","64\n",":: loss @step65: 0.18274036049842834\tavg_loss_5: 0.1829717755317688\n","65\n",":: loss @step66: 0.19856372475624084\tavg_loss_5: 0.18446284234523774\n","66\n",":: loss @step67: 0.2204022854566574\tavg_loss_5: 0.18888919949531555\n","67\n",":: loss @step68: 0.2220289707183838\tavg_loss_5: 0.19661621153354644\n","68\n",":: loss @step69: 0.21144621074199677\tavg_loss_5: 0.20703631043434143\n","69\n",":: loss @step70: 0.16068178415298462\tavg_loss_5: 0.2026245951652527\n","70\n",":: loss @step71: 0.179687961935997\tavg_loss_5: 0.1988494426012039\n","71\n",":: loss @step72: 0.20773696899414062\tavg_loss_5: 0.19631637930870055\n","72\n",":: loss @step73: 0.22770698368549347\tavg_loss_5: 0.1974519819021225\n","73\n",":: loss @step74: 0.21631690859794617\tavg_loss_5: 0.19842612147331237\n","74\n",":: loss @step75: 0.1872454285621643\tavg_loss_5: 0.20373885035514833\n","75\n",":: loss @step76: 0.2108759880065918\tavg_loss_5: 0.20997645556926728\n","76\n",":: loss @step77: 0.237054705619812\tavg_loss_5: 0.21584000289440156\n","77\n",":: loss @step78: 0.17713306844234467\tavg_loss_5: 0.2057252198457718\n","78\n",":: loss @step79: 0.2209024876356125\tavg_loss_5: 0.20664233565330506\n","79\n",":: loss @step80: 0.23416964709758759\tavg_loss_5: 0.2160271793603897\n","80\n",":: loss @step81: 0.21562448143959045\tavg_loss_5: 0.21697687804698945\n","81\n",":: loss @step82: 0.19279637932777405\tavg_loss_5: 0.20812521278858184\n","82\n",":: loss @step83: 0.24225814640522003\tavg_loss_5: 0.22115022838115692\n","83\n",":: loss @step84: 0.22371560335159302\tavg_loss_5: 0.22171285152435302\n","84\n",":: loss @step85: 0.22384335100650787\tavg_loss_5: 0.2196475923061371\n","85\n",":: loss @step86: 0.22816739976406097\tavg_loss_5: 0.22215617597103118\n","86\n",":: loss @step87: 0.2231328785419464\tavg_loss_5: 0.22822347581386565\n","87\n",":: loss @step88: 0.24545922875404358\tavg_loss_5: 0.22886369228363038\n","88\n",":: loss @step89: 0.1437893807888031\tavg_loss_5: 0.2128784477710724\n","89\n",":: loss @step90: 0.2399577796459198\tavg_loss_5: 0.21610133349895477\n","90\n",":: loss @step91: 0.18008071184158325\tavg_loss_5: 0.20648399591445923\n","91\n",":: loss @step92: 0.28412100672721863\tavg_loss_5: 0.21868162155151366\n","92\n",":: loss @step93: 0.21104179322719574\tavg_loss_5: 0.2117981344461441\n","93\n",":: loss @step94: 0.20718272030353546\tavg_loss_5: 0.22447680234909057\n","94\n",":: loss @step95: 0.2381201982498169\tavg_loss_5: 0.22410928606987\n","95\n",":: loss @step96: 0.14016181230545044\tavg_loss_5: 0.21612550616264342\n","96\n",":: loss @step97: 0.1619187742471695\tavg_loss_5: 0.1916850596666336\n","97\n",":: loss @step98: 0.19064708054065704\tavg_loss_5: 0.18760611712932587\n","98\n",":: loss @step99: 0.15606170892715454\tavg_loss_5: 0.1773819148540497\n","99\n",":: loss @step100: 0.1622198522090912\tavg_loss_5: 0.16220184564590454\n","100\n",":: loss @step101: 0.23378418385982513\tavg_loss_5: 0.1809263199567795\n","101\n",":: loss @step102: 0.15552175045013428\tavg_loss_5: 0.17964691519737244\n","102\n",":: loss @step103: 0.15317440032958984\tavg_loss_5: 0.172152379155159\n","103\n",":: loss @step104: 0.1649167686700821\tavg_loss_5: 0.1739233911037445\n","104\n",":: loss @step105: 0.16790778934955597\tavg_loss_5: 0.17506097853183747\n","105\n",":: loss @step106: 0.21204107999801636\tavg_loss_5: 0.1707123577594757\n","106\n",":: loss @step107: 0.1715032011270523\tavg_loss_5: 0.17390864789485933\n","107\n",":: loss @step108: 0.2288082391023636\tavg_loss_5: 0.18903541564941406\n","108\n",":: loss @step109: 0.21592587232589722\tavg_loss_5: 0.19923723638057708\n","109\n",":: loss @step110: 0.17187507450580597\tavg_loss_5: 0.20003069341182708\n","110\n",":: loss @step111: 0.25358012318611145\tavg_loss_5: 0.2083385020494461\n","111\n",":: loss @step112: 0.19044846296310425\tavg_loss_5: 0.21212755441665648\n","112\n",":: loss @step113: 0.20310454070568085\tavg_loss_5: 0.20698681473731995\n","113\n",":: loss @step114: 0.2143269032239914\tavg_loss_5: 0.20666702091693878\n","114\n",":: loss @step115: 0.15991288423538208\tavg_loss_5: 0.204274582862854\n","115\n",":: loss @step116: 0.21556749939918518\tavg_loss_5: 0.19667205810546876\n","116\n",":: loss @step117: 0.2023739367723465\tavg_loss_5: 0.1990571528673172\n","117\n",":: loss @step118: 0.21068322658538818\tavg_loss_5: 0.20057289004325868\n","118\n",":: loss @step119: 0.1900562047958374\tavg_loss_5: 0.19571875035762787\n","119\n",":: loss @step120: 0.1827186495065689\tavg_loss_5: 0.20027990341186525\n","120\n",":: loss @step121: 0.1464165300130844\tavg_loss_5: 0.18644970953464507\n","121\n",":: loss @step122: 0.2069510817527771\tavg_loss_5: 0.1873651385307312\n","122\n",":: loss @step123: 0.24639752507209778\tavg_loss_5: 0.19450799822807313\n","123\n",":: loss @step124: 0.2072446197271347\tavg_loss_5: 0.1979456812143326\n","124\n",":: loss @step125: 0.20817892253398895\tavg_loss_5: 0.2030377358198166\n","125\n",":: loss @step126: 0.21093624830245972\tavg_loss_5: 0.21594167947769166\n","126\n",":: loss @step127: 0.19454579055309296\tavg_loss_5: 0.21346062123775483\n","127\n",":: loss @step128: 0.19787344336509705\tavg_loss_5: 0.20375580489635467\n","128\n",":: loss @step129: 0.23275136947631836\tavg_loss_5: 0.2088571548461914\n","129\n",":: loss @step130: 0.20700214803218842\tavg_loss_5: 0.2086217999458313\n","130\n",":: loss @step131: 0.16825391352176666\tavg_loss_5: 0.2000853329896927\n","131\n",":: loss @step132: 0.20528334379196167\tavg_loss_5: 0.20223284363746644\n","132\n",":: loss @step133: 0.2045569270849228\tavg_loss_5: 0.20356954038143157\n","133\n",":: loss @step134: 0.20983152091503143\tavg_loss_5: 0.1989855706691742\n","134\n",":: loss @step135: 0.24390897154808044\tavg_loss_5: 0.2063669353723526\n","135\n",":: loss @step136: 0.20351332426071167\tavg_loss_5: 0.2134188175201416\n","136\n",":: loss @step137: 0.22394001483917236\tavg_loss_5: 0.21715015172958374\n","137\n",":: loss @step138: 0.1459345519542694\tavg_loss_5: 0.20542567670345308\n","138\n",":: loss @step139: 0.20133332908153534\tavg_loss_5: 0.20372603833675385\n","139\n",":: loss @step140: 0.17278459668159485\tavg_loss_5: 0.18950116336345674\n","140\n",":: loss @step141: 0.25393885374069214\tavg_loss_5: 0.19958626925945283\n","141\n",":: loss @step142: 0.23258838057518005\tavg_loss_5: 0.20131594240665435\n","142\n",":: loss @step143: 0.18608199059963226\tavg_loss_5: 0.20934543013572693\n","143\n",":: loss @step144: 0.24869941174983978\tavg_loss_5: 0.21881864666938783\n","144\n",":: loss @step145: 0.16286833584308624\tavg_loss_5: 0.2168353945016861\n","145\n",":: loss @step146: 0.21261046826839447\tavg_loss_5: 0.20856971740722657\n","146\n",":: loss @step147: 0.1797407865524292\tavg_loss_5: 0.1980001986026764\n","147\n",":: loss @step148: 0.1647900938987732\tavg_loss_5: 0.19374181926250458\n","148\n",":: loss @step149: 0.21620124578475952\tavg_loss_5: 0.18724218606948853\n","149\n",":: loss @step150: 0.18216514587402344\tavg_loss_5: 0.19110154807567598\n","150\n",":: loss @step151: 0.1942058503627777\tavg_loss_5: 0.1874206244945526\n","151\n",":: loss @step152: 0.23425129055976868\tavg_loss_5: 0.19832272529602052\n","152\n",":: loss @step153: 0.2118765115737915\tavg_loss_5: 0.20774000883102417\n","153\n",":: loss @step154: 0.18580183386802673\tavg_loss_5: 0.2016601264476776\n","154\n",":: loss @step155: 0.19314974546432495\tavg_loss_5: 0.20385704636573793\n","155\n",":: loss @step156: 0.20512887835502625\tavg_loss_5: 0.2060416519641876\n","156\n",":: loss @step157: 0.17988348007202148\tavg_loss_5: 0.1951680898666382\n","157\n",":: loss @step158: 0.23011937737464905\tavg_loss_5: 0.1988166630268097\n","158\n",":: loss @step159: 0.16958457231521606\tavg_loss_5: 0.19557321071624756\n","159\n",":: loss @step160: 0.24355757236480713\tavg_loss_5: 0.205654776096344\n","160\n",":: loss @step161: 0.221461683511734\tavg_loss_5: 0.20892133712768554\n","161\n",":: loss @step162: 0.3148777484893799\tavg_loss_5: 0.23592019081115723\n","162\n",":: loss @step163: 0.16878043115139008\tavg_loss_5: 0.22365240156650543\n","163\n",":: loss @step164: 0.25118157267570496\tavg_loss_5: 0.2399718016386032\n","164\n",":: loss @step165: 0.21913552284240723\tavg_loss_5: 0.23508739173412324\n","165\n",":: loss @step166: 0.19112448394298553\tavg_loss_5: 0.22901995182037355\n","166\n",":: loss @step167: 0.2709280848503113\tavg_loss_5: 0.22023001909255982\n","167\n",":: loss @step168: 0.15609830617904663\tavg_loss_5: 0.21769359409809114\n","168\n",":: loss @step169: 0.15915857255458832\tavg_loss_5: 0.1992889940738678\n","169\n",":: loss @step170: 0.20716115832328796\tavg_loss_5: 0.19689412117004396\n","170\n",":: loss @step171: 0.1951383352279663\tavg_loss_5: 0.1976968914270401\n","171\n",":: loss @step172: 0.19607967138290405\tavg_loss_5: 0.18272720873355866\n","172\n",":: loss @step173: 0.1686282902956009\tavg_loss_5: 0.1852332055568695\n","173\n",":: loss @step174: 0.19417399168014526\tavg_loss_5: 0.1922362893819809\n","174\n",":: loss @step175: 0.13496771454811096\tavg_loss_5: 0.1777976006269455\n","175\n",":: loss @step176: 0.2550854980945587\tavg_loss_5: 0.18978703320026397\n","176\n",":: loss @step177: 0.20942169427871704\tavg_loss_5: 0.19245543777942659\n","177\n",":: loss @step178: 0.20792904496192932\tavg_loss_5: 0.20031558871269226\n","178\n",":: loss @step179: 0.19715572893619537\tavg_loss_5: 0.2009119361639023\n","179\n",":: loss @step180: 0.21307146549224854\tavg_loss_5: 0.2165326863527298\n","180\n",":: loss @step181: 0.20073258876800537\tavg_loss_5: 0.20566210448741912\n","181\n",":: loss @step182: 0.203495055437088\tavg_loss_5: 0.2044767767190933\n","182\n",":: loss @step183: 0.20085182785987854\tavg_loss_5: 0.20306133329868317\n","183\n",":: loss @step184: 0.22333192825317383\tavg_loss_5: 0.20829657316207886\n","184\n",":: loss @step185: 0.22359713912010193\tavg_loss_5: 0.21040170788764953\n","185\n",":: loss @step186: 0.21717476844787598\tavg_loss_5: 0.21369014382362367\n","186\n",":: loss @step187: 0.2045752853155136\tavg_loss_5: 0.21390618979930878\n","187\n",":: loss @step188: 0.24023506045341492\tavg_loss_5: 0.22178283631801604\n","188\n",":: loss @step189: 0.19892919063568115\tavg_loss_5: 0.2169022887945175\n","189\n",":: loss @step190: 0.1773630678653717\tavg_loss_5: 0.20765547454357147\n","190\n",":: loss @step191: 0.17467597126960754\tavg_loss_5: 0.1991557151079178\n","191\n",":: loss @step192: 0.24266350269317627\tavg_loss_5: 0.2067733585834503\n","192\n",":: loss @step193: 0.20844429731369019\tavg_loss_5: 0.20041520595550538\n","193\n",":: loss @step194: 0.22407732903957367\tavg_loss_5: 0.20544483363628388\n","194\n",":: loss @step195: 0.17280632257461548\tavg_loss_5: 0.20453348457813264\n","195\n",":: loss @step196: 0.2181863784790039\tavg_loss_5: 0.2132355660200119\n","196\n",":: loss @step197: 0.1974036991596222\tavg_loss_5: 0.2041836053133011\n","197\n",":: loss @step198: 0.17707210779190063\tavg_loss_5: 0.19790916740894318\n","198\n",":: loss @step199: 0.17921499907970428\tavg_loss_5: 0.1889367014169693\n","199\n",":: loss @step200: 0.19402456283569336\tavg_loss_5: 0.1931803494691849\n","200\n",":: loss @step201: 0.18950608372688293\tavg_loss_5: 0.1874442905187607\n","201\n",":: loss @step202: 0.20910997688770294\tavg_loss_5: 0.18978554606437684\n","202\n",":: loss @step203: 0.18419089913368225\tavg_loss_5: 0.19120930433273314\n","203\n",":: loss @step204: 0.18288478255271912\tavg_loss_5: 0.1919432610273361\n","204\n",":: loss @step205: 0.23608960211277008\tavg_loss_5: 0.20035626888275146\n","205\n",":: loss @step206: 0.2376936972141266\tavg_loss_5: 0.2099937915802002\n","206\n",":: loss @step207: 0.28695225715637207\tavg_loss_5: 0.225562247633934\n","207\n",":: loss @step208: 0.13572025299072266\tavg_loss_5: 0.2158681184053421\n","208\n",":: loss @step209: 0.1549157202243805\tavg_loss_5: 0.2102743059396744\n","209\n",":: loss @step210: 0.2006530612707138\tavg_loss_5: 0.20318699777126312\n","210\n",":: loss @step211: 0.2692510485649109\tavg_loss_5: 0.20949846804141997\n","211\n",":: loss @step212: 0.23859748244285583\tavg_loss_5: 0.19982751309871674\n","212\n",":: loss @step213: 0.15929868817329407\tavg_loss_5: 0.20454320013523103\n","213\n",":: loss @step214: 0.2556374669075012\tavg_loss_5: 0.22468754947185515\n","214\n",":: loss @step215: 0.20897753536701202\tavg_loss_5: 0.2263524442911148\n","215\n",":: loss @step216: 0.18428973853588104\tavg_loss_5: 0.20936018228530884\n","216\n",":: loss @step217: 0.17047327756881714\tavg_loss_5: 0.1957353413105011\n","217\n",":: loss @step218: 0.23610682785511017\tavg_loss_5: 0.21109696924686433\n","218\n",":: loss @step219: 0.12074120342731476\tavg_loss_5: 0.18411771655082704\n","219\n",":: loss @step220: 0.13744398951530457\tavg_loss_5: 0.16981100738048555\n","220\n",":: loss @step221: 0.28737178444862366\tavg_loss_5: 0.19042741656303405\n","221\n",":: loss @step222: 0.12331011891365051\tavg_loss_5: 0.18099478483200074\n","222\n",":: loss @step223: 0.1920749545097351\tavg_loss_5: 0.17218841016292571\n","223\n",":: loss @step224: 0.218411386013031\tavg_loss_5: 0.19172244668006896\n","224\n",":: loss @step225: 0.18399259448051453\tavg_loss_5: 0.20103216767311097\n","225\n",":: loss @step226: 0.2158549576997757\tavg_loss_5: 0.18672880232334138\n","226\n",":: loss @step227: 0.1159924566745758\tavg_loss_5: 0.18526526987552644\n","227\n",":: loss @step228: 0.20677505433559418\tavg_loss_5: 0.18820528984069823\n","228\n",":: loss @step229: 0.1931169331073761\tavg_loss_5: 0.18314639925956727\n","229\n",":: loss @step230: 0.17904232442378998\tavg_loss_5: 0.18215634524822236\n","230\n",":: loss @step231: 0.15554381906986237\tavg_loss_5: 0.17009411752223969\n","231\n",":: loss @step232: 0.20754513144493103\tavg_loss_5: 0.18840465247631072\n","232\n",":: loss @step233: 0.19330474734306335\tavg_loss_5: 0.18571059107780458\n","233\n",":: loss @step234: 0.1358095407485962\tavg_loss_5: 0.17424911260604858\n","234\n",":: loss @step235: 0.2441796064376831\tavg_loss_5: 0.18727656900882722\n","235\n",":: loss @step236: 0.174294576048851\tavg_loss_5: 0.19102672040462493\n","236\n",":: loss @step237: 0.16370606422424316\tavg_loss_5: 0.18225890696048735\n","237\n",":: loss @step238: 0.19776451587677002\tavg_loss_5: 0.1831508606672287\n","238\n",":: loss @step239: 0.20681333541870117\tavg_loss_5: 0.1973516196012497\n","239\n",":: loss @step240: 0.20958910882472992\tavg_loss_5: 0.19043352007865905\n","240\n",":: loss @step241: 0.20435020327568054\tavg_loss_5: 0.19644464552402496\n","241\n",":: loss @step242: 0.13037526607513428\tavg_loss_5: 0.18977848589420318\n","242\n",":: loss @step243: 0.17173554003238678\tavg_loss_5: 0.18457269072532653\n","243\n",":: loss @step244: 0.12407389283180237\tavg_loss_5: 0.16802480220794677\n","244\n",":: loss @step245: 0.18557211756706238\tavg_loss_5: 0.16322140395641327\n","245\n",":: loss @step246: 0.1898687183856964\tavg_loss_5: 0.16032510697841645\n","246\n",":: loss @step247: 0.13290008902549744\tavg_loss_5: 0.16083007156848908\n","247\n",":: loss @step248: 0.14392665028572083\tavg_loss_5: 0.1552682936191559\n","248\n",":: loss @step249: 0.1487622857093811\tavg_loss_5: 0.16020597219467164\n","249\n",":: loss @step250: 0.17107263207435608\tavg_loss_5: 0.15730607509613037\n","250\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 53.76x53.76 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# pretrain_apn.py\n","\n","import shutil\n","import cv2\n","import imageio\n","import os\n","import numpy as np\n","import sys\n","import torch\n","import time\n","import torch.optim as optim\n","import torch.backends.cudnn as cudnn\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","\n","sys.path.append('.')  # noqa: E402\n","from torch.autograd import Variable\n","\n","\n","\n","def log(msg):\n","    open('build/core.log', 'a').write(f'[{time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())}]\\t'+msg+'\\n'), print(msg)\n","\n","\n","def random_sample(dataloader):\n","    for batch_idx, (inputs, _) in enumerate(dataloader, 0):\n","        return inputs[0].cuda()\n","\n","\n","def save_img(x, path, annotation=''):\n","    fig = plt.gcf()  # generate outputs\n","    plt.imshow(PlantDataset.tensor_to_img(x[0]), aspect='equal'), plt.axis('off'), fig.set_size_inches(224/100.0/3.0, 224/100.0/3.0)\n","    plt.gca().xaxis.set_major_locator(plt.NullLocator()), plt.gca().yaxis.set_major_locator(plt.NullLocator()), plt.subplots_adjust(top=1, bottom=0, left=0, right=1, hspace=0, wspace=0), plt.margins(0, 0)\n","    plt.text(0, 0, annotation, color='white', size=4, ha=\"left\", va=\"top\", bbox=dict(boxstyle=\"square\", ec='black', fc='black'))\n","    plt.savefig(path, dpi=300, pad_inches=0)    # visualize masked image\n","\n","\n","def run(pretrained_backbone=None):\n","    net = RACNN(num_classes=6).cuda()\n","    if pretrained_backbone:  # Using pretrained backbone for apn pretraining\n","        state_dict = torch.load(pretrained_backbone).state_dict()\n","        net.b1.load_state_dict(state_dict)\n","        net.b2.load_state_dict(state_dict)\n","        net.b3.load_state_dict(state_dict)\n","\n","    cudnn.benchmark = True\n","\n","    params = list(net.apn1.parameters()) + list(net.apn2.parameters())\n","    optimizer = optim.SGD(params, lr=0.001, momentum=0.9)\n","\n","    data_set = get_plant_loader()\n","    trainloader = torch.utils.data.DataLoader(data_set[\"train\"], batch_size=4, shuffle=True)\n","    validationloader = torch.utils.data.DataLoader(data_set[\"validation\"], batch_size=8, shuffle=False)\n","    sample = random_sample(validationloader)\n","    \n","    net.mode(\"pretrain_apn\")\n","\n","    def avg(x): return sum(x)/len(x)\n","    for epoch in range(1):\n","        losses = []\n","        for step, (inputs, _) in enumerate(trainloader, 0):\n","\n","            loss = net.echo(inputs, optimizer)\n","            losses.append(loss)\n","            avg_loss = avg(losses[-5 if len(losses) > 5 else -len(losses):])\n","            print(f':: loss @step{step:2d}: {loss}\\tavg_loss_5: {avg_loss}')\n","\n","            if step % 2 == 0 or step < 5:  # check point\n","                _, _, _, resized = net(sample.unsqueeze(0))\n","                x1, x2 = resized[0].data, resized[1].data\n","                # visualize cropped inputs\n","                save_img(x1, path=f'build/.cache/step_{step}@2x.jpg', annotation=f'loss = {avg_loss:.7f}, step = {step}')\n","                save_img(x2, path=f'build/.cache/step_{step}@4x.jpg', annotation=f'loss = {avg_loss:.7f}, step = {step}')\n","            print(step)\n","            if step >= 250:  # 128 steps is enough for pretraining\n","                torch.save(net.state_dict(), f'build/racnn_pretrained.pt')\n","                return\n","\n","\n","def build_gif(pattern='@2x', gif_name='pretrain_apn_EfficientNet', cache_path='build/.cache'):\n","    # generate a gif, enjoy XD\n","    files = [x for x in os.listdir(cache_path) if pattern in x]\n","    files.sort(key=lambda x: int(x.split('@')[0].split('_')[-1]))\n","    gif_images = [imageio.imread(f'{cache_path}/{img_file}') for img_file in files]\n","    imageio.mimsave(f\"build/{gif_name}{pattern}-{int(time.time())}.gif\", gif_images, fps=8)\n","\n","\n","def clean(path='build/.cache/'):\n","    print(' :: Cleaning cache dir ...')\n","    if os.path.exists(path):\n","        shutil.rmtree(path)\n","    os.makedirs(path)\n","\n","\n","if __name__ == \"__main__\":\n","    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","    clean()\n","    run(pretrained_backbone='build/efficientNet_b0_ImageNet.pt')\n","    build_gif(pattern='@2x', gif_name='pretrain_apn_EfficientNet')\n","    build_gif(pattern='@4x', gif_name='pretrain_apn_EfficientNet')\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GdXYDa7cN4Vt","outputId":"4b8f9ff8-50d2-48bc-9d0f-2ffa218fa19e"},"outputs":[{"name":"stdout","output_type":"stream","text":[" :: Cleaning cache dir ...\n"," :: Start training with build/racnn_pretrained.pt\n"," :: Switch to backbone\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\gabri\\AppData\\Local\\Temp/ipykernel_8052/3993585667.py:89: DeprecationWarning: 'saved_variables' is deprecated; use 'saved_tensors'\n","  images, ret_tensor = self.saved_variables[0], self.saved_variables[1]\n"]},{"name":"stdout","output_type":"stream","text":[":: loss @step(20/1677)-epoch0: 2.0717616081\tavg_loss_20: 2.2375411391\n",":: loss @step(40/1677)-epoch0: 1.9287784100\tavg_loss_20: 1.9781411529\n",":: loss @step(60/1677)-epoch0: 1.7712283134\tavg_loss_20: 1.8678820252\n",":: loss @step(80/1677)-epoch0: 1.8072191477\tavg_loss_20: 1.7979314089\n",":: loss @step(100/1677)-epoch0: 1.6437041759\tavg_loss_20: 1.7447265506\n",":: loss @step(120/1677)-epoch0: 1.5965900421\tavg_loss_20: 1.6680547893\n",":: loss @step(140/1677)-epoch0: 1.5982102156\tavg_loss_20: 1.6467815220\n",":: loss @step(160/1677)-epoch0: 1.5285352468\tavg_loss_20: 1.5718187213\n",":: loss @step(180/1677)-epoch0: 1.5758796930\tavg_loss_20: 1.5815971315\n",":: loss @step(200/1677)-epoch0: 1.5082687140\tavg_loss_20: 1.5581459403\n",":: loss @step(220/1677)-epoch0: 1.4971156120\tavg_loss_20: 1.5160484850\n",":: loss @step(240/1677)-epoch0: 1.7200260162\tavg_loss_20: 1.4785947561\n",":: loss @step(260/1677)-epoch0: 1.3355091810\tavg_loss_20: 1.4331928074\n",":: loss @step(280/1677)-epoch0: 1.5916762352\tavg_loss_20: 1.4402582943\n",":: loss @step(300/1677)-epoch0: 1.6408413649\tavg_loss_20: 1.4553701997\n",":: loss @step(320/1677)-epoch0: 1.2987825871\tavg_loss_20: 1.3798641682\n",":: loss @step(340/1677)-epoch0: 1.4851896763\tavg_loss_20: 1.3821180046\n",":: loss @step(360/1677)-epoch0: 1.3812229633\tavg_loss_20: 1.3390419245\n",":: loss @step(380/1677)-epoch0: 1.2495348454\tavg_loss_20: 1.3543618262\n",":: loss @step(400/1677)-epoch0: 1.5734996796\tavg_loss_20: 1.3564128816\n",":: loss @step(420/1677)-epoch0: 1.2412730455\tavg_loss_20: 1.3669015944\n",":: loss @step(440/1677)-epoch0: 1.2353156805\tavg_loss_20: 1.3389791012\n"]}],"source":["# forge.py\n","\n","import imageio\n","import os\n","import shutil\n","import sys\n","from scipy.sparse import data\n","import torch\n","import time\n","import torch.optim as optim\n","import torch.backends.cudnn as cudnn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","\n","sys.path.append('.')  # noqa: E402\n","\n","\n","\n","def avg(x): return sum(x)/len(x)\n","\n","def train(net, dataloader, optimizer, epoch, _type):\n","    assert _type in ['apn', 'backbone']\n","    losses = 0\n","    net.mode(_type), log(f' :: Switch to {_type}')  # switch loss type\n","    for step, (inputs, targets) in enumerate(dataloader, 0):\n","        loss = net.echo(inputs, targets, optimizer)\n","        losses += loss\n","\n","        if step % 20 == 0 and step != 0:\n","            avg_loss = losses/20\n","            log(f':: loss @step({step:2d}/{len(dataloader)})-epoch{epoch}: {loss:.10f}\\tavg_loss_20: {avg_loss:.10f}')\n","            losses = 0\n","\n","    return avg_loss\n","\n","\n","def test(net, dataloader):\n","    correct = [0,0,0]\n","    cnt = 0\n","    avg_accuracy = 0\n","\n","    for step, (inputs, labels) in enumerate(dataloader, 0):\n","        inputs, labels = Variable(inputs).cuda(), Variable(labels).cuda()\n","        cnt += inputs.size(0)\n","        with torch.no_grad():\n","            outputs, _, _, _ = net(inputs)\n","            for idx, logits in enumerate(outputs):\n","                logits = torch.sigmoid(logits)\n","                logits[logits >= 0.5 ] = 1\n","                logits[logits < 0.5 ] = 0\n","                #correct_summary[f'clsf-{idx}']['top-1'] +=  torch.all(torch.eq(logits, labels),  dim=1).sum()  # top-1\n","                correct[idx] += torch.all(torch.eq(logits, labels),  dim=1).sum()\n","                #correct_summary[f'clsf-{idx}']['top-5'] += torch.eq(logits.topk(max((1, 5)), 1, True, True)[1], labels.view(-1, 1)).sum().float().item()  # top-5\n","    for id,value in enumerate(correct):\n","      correct[id] = value/cnt\n","      avg_accuracy += correct[id]\n","    \n","    return avg_accuracy/3, correct\n","\n","\n","def run(pretrained_model):\n","    accuracy = 0\n","    log(f' :: Start training with {pretrained_model}')\n","    net = RACNN(num_classes=6).cuda()\n","    net.load_state_dict(torch.load(pretrained_model))\n","    cudnn.benchmark = True\n","\n","    #ognuna delle 3 cnn del modello parte con i valori della cnn pre addestrata e poi ognuna si specializza\n","    #con i propri parametri\n","    cls_params = list(net.b1.parameters()) + list(net.b2.parameters()) + list(net.b3.parameters()) + \\\n","        list(net.classifier1.parameters()) + list(net.classifier2.parameters()) + list(net.classifier3.parameters())\n","    apn_params = list(net.apn1.parameters()) + list(net.apn2.parameters())\n","\n","    cls_opt = optim.SGD(cls_params, lr=0.001, momentum=0.9)\n","    #TODO da modificare in lr=1e-6\n","    apn_opt = optim.SGD(apn_params, lr=0.0001)\n","\n","    data_set = get_plant_loader()\n","    trainloader = torch.utils.data.DataLoader(data_set[\"train\"], batch_size=10, shuffle=True)\n","    validationloader = torch.utils.data.DataLoader(data_set[\"validation\"], batch_size=10, shuffle=False)\n","    sample = random_sample(validationloader)\n","\n","    cls_losses = []\n","    rank_losses = []\n","    training_accuracies = []\n","    validation_accuracies = []\n","\n","    cls1_accuracies_valid = []\n","    cls2_accuracies_valid = []\n","    cls3_accuracies_valid = []\n","\n","    cls1_accuracies_train = []\n","    cls2_accuracies_train = []\n","    cls3_accuracies_train = []\n","\n","    #15\n","    for epoch in range(15):\n","        net.train()\n","        \n","        cls_loss = train(net, trainloader, cls_opt, epoch, 'backbone')\n","        rank_loss = train(net, trainloader, apn_opt, epoch, 'apn')\n","        cls_losses.append(cls_loss)\n","        rank_losses.append(rank_loss)\n","\n","        net.eval()\n","        log(' :: Testing on validation set ...')\n","        temp_accuracy, valid_corrects = test(net, validationloader)\n","        validation_accuracies.append(temp_accuracy)\n","        cls1_accuracies_valid.append(valid_corrects[0])\n","        cls2_accuracies_valid.append(valid_corrects[1])\n","        cls3_accuracies_valid.append(valid_corrects[2])\n","\n","        log(' :: Testing on training set ...')\n","        train_accuracy, train_corrects = test(net, trainloader)\n","        training_accuracies.append(train_accuracy)\n","        cls1_accuracies_train.append(train_corrects[0])\n","        cls2_accuracies_train.append(train_corrects[1])\n","        cls3_accuracies_train.append(train_corrects[2])\n","\n","\n","        # visualize cropped inputs\n","        _, _, _, resized = net(sample.unsqueeze(0))\n","        x1, x2 = resized[0].data, resized[1].data\n","        save_img(x1, path=f'build/.cache/epoch_{epoch}@2x.jpg', annotation=f'cls_loss = {cls_loss:.7f}, rank_loss = {rank_loss:.7f}')\n","        save_img(x2, path=f'build/.cache/epoch_{epoch}@4x.jpg', annotation=f'cls_loss = {cls_loss:.7f}, rank_loss = {rank_loss:.7f}')\n","\n","        # save model per 10 epoches\n","        if temp_accuracy > accuracy:\n","            accuracy = temp_accuracy\n","            stamp = f'e{epoch}{int(time.time())}'\n","            torch.save(net.state_dict(), f'build/racnn_efficientNetB0.pt')\n","            log(f' :: Saved model dict as:\\tbuild/racnn_efficientNetB0.pt')\n","            torch.save(cls_opt.state_dict(), f'build/cls_optimizer.pt')\n","            torch.save(apn_opt.state_dict(), f'build/apn_optimizer.pt')\n","\n","    np.savetxt(f'logs/racnn-cls-loss.csv',cls_losses,'%.10f',',')\n","    np.savetxt(f'logs/racnn-rank-loss.csv',rank_losses,'%.10f',',')\n","    np.savetxt(f'logs/racnn-training-accuracy.csv',training_accuracies,'%.10f',',')\n","    np.savetxt(f'logs/racnn-validation-accuracy.csv',validation_accuracies,'%.10f',',')\n","    np.savetxt(f'logs/racnn-cls1-validation-accuracy.csv',cls1_accuracies_valid,'%.10f',',')\n","    np.savetxt(f'logs/racnn-cls2-validation-accuracy.csv',cls2_accuracies_valid,'%.10f',',')\n","    np.savetxt(f'logs/racnn-cls3-validation-accuracy.csv',cls3_accuracies_valid,'%.10f',',')\n","    np.savetxt(f'logs/racnn-cls1-training-accuracy.csv',cls1_accuracies_train,'%.10f',',')\n","    np.savetxt(f'logs/racnn-cls2-training-accuracy.csv',cls2_accuracies_train,'%.10f',',')\n","    np.savetxt(f'logs/racnn-cls3-training-accuracy.csv',cls3_accuracies_train,'%.10f',',')\n","      \n","      \n","\n","\n","if __name__ == \"__main__\":\n","    clean()\n","    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","    path = 'logs'\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","    #RACNN con backbone e apn pre addestrate\n","    run(pretrained_model='build/racnn_pretrained.pt')\n","    build_gif(pattern='@2x', gif_name='racnn_efficientNet')\n","    build_gif(pattern='@4x', gif_name='racnn_efficientNet')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WeCjoxC1Opnv"},"outputs":[],"source":["# racnn plots\n","\n","#!cp -r /content/drive/MyDrive/logs/13_01_2022 /content/logs \n","\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","cls_loss = np.loadtxt(f'logs/racnn-cls-loss.csv',delimiter=',')\n","rank_loss = np.loadtxt(f'logs/racnn-rank-loss.csv',delimiter=',')\n","training_accuracy = np.loadtxt(f'logs/racnn-training-accuracy.csv',delimiter=',')\n","validation_accuracy = np.loadtxt(f'logs/racnn-validation-accuracy.csv',delimiter=',')\n","\n","cls1_valid_accuracy = np.loadtxt(f'logs/racnn-cls1-validation-accuracy.csv',delimiter=',')\n","cls1_train_accuracy = np.loadtxt(f'logs/racnn-cls1-training-accuracy.csv',delimiter=',')\n","\n","cls2_valid_accuracy = np.loadtxt(f'logs/racnn-cls2-validation-accuracy.csv',delimiter=',')\n","cls2_train_accuracy = np.loadtxt(f'logs/racnn-cls2-training-accuracy.csv',delimiter=',')\n","\n","cls3_valid_accuracy = np.loadtxt(f'logs/racnn-cls3-validation-accuracy.csv',delimiter=',')\n","cls3_train_accuracy = np.loadtxt(f'logs/racnn-cls3-training-accuracy.csv',delimiter=',')\n","\n","n_epochs = 15\n","x = np.linspace(1,n_epochs,n_epochs)\n","\n","desc = \"APN lr=0.0001 without momentum\"\n","\n","fig = plt.figure(figsize=(10, 7))\n","plt.plot(x,cls_loss, color='red', label='cls loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.title(f'RACNN - CLS Loss - {desc}')\n","plt.legend()\n","plt.show()\n","fig.savefig(f'plots/racnn-cls-loss.png')\n","\n","fig = plt.figure(figsize=(10, 7))\n","plt.plot(x,rank_loss, color='red', label='rank loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.title(f'RACNN - Rank Loss - {desc}')\n","plt.legend()\n","plt.show()\n","fig.savefig(f'plots/racnn-rank-loss.png')\n","\n","fig = plt.figure(figsize=(10, 7))\n","plt.plot(x,cls1_valid_accuracy, color='red', label='cls1')\n","plt.plot(x,cls2_valid_accuracy, color='green', label='cls2')\n","plt.plot(x,cls3_valid_accuracy, color='blue', label='cls3')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.title(f'RACNN - CLS Accuracies - Validation Set - {desc}')\n","plt.legend()\n","plt.show()\n","fig.savefig(f'plots/racnn-cls-valid-accuracy.png')\n","\n","fig = plt.figure(figsize=(10, 7))\n","plt.plot(x,cls1_train_accuracy, color='red', label='cls1')\n","plt.plot(x,cls2_train_accuracy, color='green', label='cls2')\n","plt.plot(x,cls3_train_accuracy, color='blue', label='cls3')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.title(f'RACNN - CLS Accuracies - Training Set - {desc}')\n","plt.legend()\n","plt.show()\n","fig.savefig(f'plots/racnn-cls-train-accuracy.png')\n","\n","fig = plt.figure(figsize=(10, 7))\n","plt.plot(x,validation_accuracy, color='orange', label='validation set')\n","plt.plot(x,training_accuracy, color='green', label='training set')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.title(f'RACNN - Accuracy - {desc}')\n","plt.legend()\n","plt.show()\n","fig.savefig(f'plots/racnn-accuracy.png')\n","\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"RACNN.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.10"}},"nbformat":4,"nbformat_minor":0}

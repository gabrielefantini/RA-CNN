{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"RGPKtTer9SpQ"},"outputs":[],"source":["!cp -r /content/drive/MyDrive/data /content/data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ckcBwjJq_Knn"},"outputs":[],"source":["!cp -r /content/drive/MyDrive/build /content/build"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"background_save":true},"id":"xQjpgylUOBVo"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","from torch.utils.data import Dataset, DataLoader\n","import cv2\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch\n","import pandas as pd\n","from sklearn.preprocessing import MultiLabelBinarizer\n","from sklearn.model_selection import train_test_split\n","import os\n","from re import L\n","import time\n","import torch.optim as optim\n","import torch.backends.cudnn as cudnn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","torch.cuda.is_available()\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"background_save":true},"id":"iBlaa0e8PEEa"},"outputs":[],"source":["class PlantDataset(Dataset):\n","        def __init__(self, df, image_dir=\"/content/data/train_images/\"):\n","\n","            std = 1. / 255.\n","            means = [109.97 / 255., 127.34 / 255., 123.88 / 255.]\n","\n","            self.image_id = df['image'].values\n","            self.labels = df.iloc[:, 1:].values\n","            self.image_dir = image_dir\n","            self.transform = transforms.Compose([\n","                transforms.ToPILImage(),\n","                transforms.Resize((224,224)),\n","                transforms.ToTensor(),\n","                transforms.Normalize(\n","                    mean=means,\n","                    std=[std]*3)\n","            ])\n","\n","        def __len__(self):\n","            return len(self.labels)\n","\n","        def __getitem__(self, idx):\n","            image_id = self.image_id[idx]\n","            label = torch.tensor(self.labels[idx].astype('int8'), dtype=torch.float32)\n","            \n","            image_path = self.image_dir + image_id\n","            image = cv2.imread(image_path)\n","            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","            \n","            image = self.transform(image)\n","\n","            return image, label\n","\n","        @staticmethod\n","        def tensor_to_img(x, imtype=np.uint8):\n","            mean = [109.97 / 255., 127.34 / 255., 123.88 / 255.]\n","            std = [1. / 255., 1. / 255., 1. / 255.]\n","\n","            if not isinstance(x, np.ndarray):\n","                if isinstance(x, torch.Tensor):  # get the data from a variable\n","                    image_tensor = x.data\n","                else:\n","                    return x\n","                image_numpy = image_tensor.cpu().float().numpy()  # convert it into a numpy array\n","                if image_numpy.shape[0] == 1:  # grayscale to RGB\n","                    image_numpy = np.tile(image_numpy, (3, 1, 1))\n","                for i in range(len(mean)):\n","                    image_numpy[i] = image_numpy[i] * std[i] + mean[i]\n","                image_numpy = image_numpy * 255\n","                image_numpy = np.transpose(image_numpy, (1, 2, 0))  # post-processing: tranpose and scaling\n","            else:  # if it is a numpy array, do nothing\n","                image_numpy = x\n","            return image_numpy.astype(imtype)\n","\n","def get_plant_loader():\n","\n","    #read data from csv\n","    df_train = pd.read_csv(f'/content/data/train.csv')\n","    #print(df_train.head())\n","\n","    #label distribution\n","    train_count = df_train['labels'].value_counts()\n","    #print(train_count)\n","\n","    #split labels\n","    df_train['labels'] = df_train['labels'].apply(lambda string: string.split(' '))\n","    #print(df_train.head(n=12))\n","\n","    #make label in binary form\n","    train_df_list = list(df_train['labels'])\n","    mlb = MultiLabelBinarizer()\n","    trainx = pd.DataFrame(mlb.fit_transform(train_df_list), columns=mlb.classes_, index=df_train.index)\n","    #print(trainx.head(n=12))\n","\n","    #concat label in binary form with image name\n","    train_data = pd.concat([df_train, trainx], axis=1).drop('labels', axis=1)\n","\n","    train, validation = train_test_split(train_data, train_size=0.9, random_state=11)\n","    #test, validation = train_test_split(remaining, test_size=0.5)\n","    print(train_data)\n","\n","    print(len(train), len(validation))\n","\n","    return {\n","        \"train\": PlantDataset(train),\n","        \"validation\": PlantDataset(validation) \n","        }"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"background_save":true},"id":"6CWFcZOPPQVD"},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/data/train.csv'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13436/802453757.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m     \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13436/802453757.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m()\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     \u001b[0mdata_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_plant_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0mtrainloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"train\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13436/3137610234.py\u001b[0m in \u001b[0;36mget_plant_loader\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;31m#read data from csv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[0mdf_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'/content/data/train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m     \u001b[1;31m#print(df_train.head())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/data/train.csv'"]}],"source":["def log(msg):\n","    open('build/core.log', 'a').write(f'[{time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())}]\\t'+msg+'\\n'), print(msg)\n","\n","def eval(net, dataloader):\n","    log(' :: Testing on validation set ...')\n","    correct_top1 = 0\n","    correct_top3 = 0\n","    correct_top5 = 0\n","    \n","    for step, (inputs, labels) in enumerate(dataloader, 0):\n","        inputs, labels = Variable(inputs).cuda(), Variable(labels).cuda()\n","\n","        with torch.no_grad():\n","            logits = net(inputs)\n","            logits = torch.sigmoid(logits)\n","            logits[logits >= 0.5 ] = 1\n","            logits[logits < 0.5 ] = 0\n","            #print(\"logits:\", logits, \"labels:\", labels)\n","\n","            #accuracy = torch.all(torch.eq(logits, labels),  dim=1).sum()/len(labels)\n","            #print(accuracy)\n","            correct_top1 += torch.all(torch.eq(logits, labels),  dim=1).sum()\n","            print(\"step: \", step)\n","            print(correct_top1/((step+1)*int(inputs.shape[0])))\n","            #correct_top1 += torch.eq(logits.topk(max((1, 1)), 1, True, True)[1], labels.view(-1, 1)).sum().float().item()\n","            #correct_top3 += torch.eq(logits.topk(max((1, 3)), 1, True, True)[1], labels.view(-1, 1)).sum().float().item()\n","            #correct_top5 += torch.eq(logits.topk(max((1, 5)), 1, True, True)[1], labels.view(-1, 1)).sum().float().item()\n","\n","        if step == 57:\n","            log(f'\\tAccuracy@top1 ({step}/{len(dataloader)}) = {correct_top1/((step+1)*int(inputs.shape[0])):.5%}')\n","            #log(f'\\tAccuracy@top3 ({step}/{len(dataloader)}) = {correct_top3/((step+1)*int(inputs.shape[0])):.5%}')\n","            #log(f'\\tAccuracy@top5 ({step}/{len(dataloader)}) = {correct_top5/((step+1)*int(inputs.shape[0])):.5%}')\n","            return correct_top1/((step+1)*int(inputs.shape[0]))\n","\n","def run():\n","    state_dict = torchvision.models.efficientnet_b0(pretrained=True).state_dict()\n","    state_dict.pop('classifier.1.weight')\n","    state_dict.pop('classifier.1.bias')\n","    net = torchvision.models.efficientnet_b0(num_classes=6).cuda()\n","\n","    state_dict['classifier.1.weight'] = net.state_dict()['classifier.1.weight']\n","    state_dict['classifier.1.bias'] = net.state_dict()['classifier.1.bias']\n","    net.load_state_dict(state_dict)\n","    cudnn.benchmark = True\n","\n","   \n","    #for param in net.parameters():\n","    #    print(type(param), param.size())\n","\n","    criterion = torch.nn.BCELoss()\n","    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n","\n","    data_set = get_plant_loader()\n","\n","    trainloader = DataLoader(data_set[\"train\"], batch_size=32, shuffle=True)\n","    validationloader = DataLoader(data_set[\"validation\"], batch_size=32, shuffle=False)\n","    \n","\n","    for epoch in range(40):  # loop over the dataset multiple times\n","            losses = 0\n","            accuracy = 0\n","\n","            for step, (inputs, labels) in enumerate(trainloader, 0):\n","                inputs, labels = Variable(inputs).cuda(), Variable(labels).cuda()\n","                optimizer.zero_grad()\n","                outputs = net(inputs)\n","\n","                outputs = torch.sigmoid(outputs)\n","                loss = criterion(outputs, labels)\n","                loss.backward()\n","                optimizer.step()\n","\n","                losses += loss\n","                if step % 20 == 0 and step != 0:\n","                    avg_loss = losses/20\n","                    log(f':: loss @step({step:2d}/{len(trainloader)})-epoch{epoch}: {loss:.10f}\\tavg_loss_20: {avg_loss:.10f}')\n","                    losses = 0\n","\n","            temp_accuracy = eval(net, validationloader)\n","            if temp_accuracy > accuracy :\n","                accuracy = temp_accuracy\n","                #stamp = f'e{epoch}{int(time.time())}'\n","                torch.save(net, f'build/efficientNet_b0_ImageNet.pt')\n","                torch.save(optimizer.state_dict, f'build/optimizer.pt')\n","\n","\n","if __name__ == \"__main__\":\n","    path = 'build'\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","    run()\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"background_save":true},"id":"rF-OA2ADM_Lu"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","import torchvision\n","\n","\n","class AttentionCropFunction(torch.autograd.Function):\n","    @staticmethod\n","    def forward(self, images, locs):\n","        \n","        def h(_x): return 1 / (1 + torch.exp(-10 * _x.float()))\n","\n","        #in_size = 224\n","        in_size = images.size()[2]\n","        #unit = tensor([[  0,   1,   2,  ..., 221, 222, 223],\n","        #               [  0,   1,   2,  ..., 221, 222, 223],\n","        #               [  0,   1,   2,  ..., 221, 222, 223],\n","        #               ...,\n","        #               [  0,   1,   2,  ..., 221, 222, 223],\n","        #               [  0,   1,   2,  ..., 221, 222, 223],\n","        #               [  0,   1,   2,  ..., 221, 222, 223]])\n","        unit = torch.stack([torch.arange(0, in_size)] * in_size)\n","        #x è 3 tensor([[[  0,   0,   0,  ...,   0,   0,   0],\n","        #               [  1,   1,   1,  ...,   1,   1,   1],\n","        #               [  2,   2,   2,  ...,   2,   2,   2],\n","        #               ...,\n","        #               [221, 221, 221,  ..., 221, 221, 221],\n","        #               [222, 222, 222,  ..., 222, 222, 222],\n","        #               [223, 223, 223,  ..., 223, 223, 223]]\n","        x = torch.stack([unit.t()] * 3)\n","        #y è 3 tensori unit\n","        y = torch.stack([unit] * 3)\n","        if isinstance(images, torch.cuda.FloatTensor):\n","            x, y = x.cuda(), y.cuda()\n","\n","        in_size = images.size()[2]\n","        ret = []\n","\n","        #processa tutte le immagini del batch, in questo caso images.size(0) è 12\n","        for i in range(images.size(0)):\n","            # locs sono tensor([[143.3381, 124.7122,  51.1188],\n","            #                   [110.5597, 137.9310,  60.7936],\n","            #                   [119.1002,  99.9731,  52.5289],\n","            #                   [113.5314, 129.4575,  46.4848],\n","            #                   [ 79.8117,  94.2794,  54.7804],\n","            #                   [117.5080, 136.6172,  63.8196],\n","            #                   [ 87.7556, 113.1122,  62.1823],\n","            #                   [131.8735, 108.5016,  63.9460],\n","            #                   [ 99.1213, 125.6766,  76.2739],\n","            #                   [ 90.7646,  89.5686,  49.1010],\n","            #                   [ 84.4682, 130.7239,  82.1634],\n","            #                   [ 88.4385,  96.4715,  42.9040]], device='cuda:0',\n","            #               grad_fn=<MulBackward0>)\n","\n","            tx, ty, tl = locs[i][0], locs[i][1], locs[i][2]\n","            \n","            #per evitare che si rimpicciolisca troppo\n","            tl = tl if tl > (in_size/3) else in_size/3\n","            #check per evitare che il crop \"sbordi\"\n","            tx = tx if tx > tl else tl\n","            tx = tx if tx < in_size-tl else in_size-tl\n","            ty = ty if ty > tl else tl\n","            ty = ty if ty < in_size-tl else in_size-tl\n","            \n","            # in un esempio, w_off, h_off, w_end, h_end 68 50 218 199\n","            w_off = int(tx-tl) if (tx-tl) > 0 else 0\n","            h_off = int(ty-tl) if (ty-tl) > 0 else 0\n","            w_end = int(tx+tl) if (tx+tl) < in_size else in_size\n","            h_end = int(ty+tl) if (ty+tl) < in_size else in_size\n","            \n","            #mk cosa fa di preciso?????\n","            mk = (h(x-w_off) - h(x-w_end)) * (h(y-h_off) - h(y-h_end))\n","            xatt = images[i] * mk\n","            xatt_cropped = xatt[:, w_off: w_end, h_off: h_end]\n","            before_upsample = Variable(xatt_cropped.unsqueeze(0))\n","            #fa l'upsampling dell'immagine croppata a 224x224\n","            xamp = F.upsample(before_upsample, size=(224, 224), mode='bilinear', align_corners=True)\n","            ret.append(xamp.data.squeeze())\n","\n","        ret_tensor = torch.stack(ret)\n","        self.save_for_backward(images, ret_tensor)\n","        return ret_tensor\n","\n","    @staticmethod\n","    def backward(self, grad_output):\n","        images, ret_tensor = self.saved_variables[0], self.saved_variables[1]\n","        in_size = 224\n","        ret = torch.Tensor(grad_output.size(0), 3).zero_()\n","        norm = -(grad_output * grad_output).sum(dim=1)\n","        x = torch.stack([torch.arange(0, in_size)] * in_size).t()\n","        y = x.t()\n","        long_size = (in_size/3*2)\n","        short_size = (in_size/3)\n","        mx = (x >= long_size).float() - (x < short_size).float()\n","        my = (y >= long_size).float() - (y < short_size).float()\n","        ml = (((x < short_size)+(x >= long_size)+(y < short_size)+(y >= long_size)) > 0).float()*2 - 1\n","\n","        mx_batch = torch.stack([mx.float()] * grad_output.size(0))\n","        my_batch = torch.stack([my.float()] * grad_output.size(0))\n","        ml_batch = torch.stack([ml.float()] * grad_output.size(0))\n","\n","        if isinstance(grad_output, torch.cuda.FloatTensor):\n","            mx_batch = mx_batch.cuda()\n","            my_batch = my_batch.cuda()\n","            ml_batch = ml_batch.cuda()\n","            ret = ret.cuda()\n","\n","        ret[:, 0] = (norm * mx_batch).sum(dim=1).sum(dim=1)\n","        ret[:, 1] = (norm * my_batch).sum(dim=1).sum(dim=1)\n","        ret[:, 2] = (norm * ml_batch).sum(dim=1).sum(dim=1)\n","        return None, ret\n","\n","\n","class AttentionCropLayer(nn.Module):\n","    \"\"\"\n","        Crop function sholud be implemented with the nn.Function.\n","        Detailed description is in 'Attention localization and amplification' part.\n","        Forward function will not changed. backward function will not opearate with autograd, but munually implemented function\n","    \"\"\"\n","\n","    def forward(self, images, locs):\n","        return AttentionCropFunction.apply(images, locs)\n","\n","\n","class RACNN(nn.Module):\n","    def __init__(self, num_classes, img_scale=448):\n","        super(RACNN, self).__init__()\n","\n","        self.b1 = torchvision.models.efficientnet_b0(num_classes=num_classes)\n","        self.b2 = torchvision.models.efficientnet_b0(num_classes=num_classes)\n","        self.b3 = torchvision.models.efficientnet_b0(num_classes=num_classes)\n","\n","        self.classifier1 = nn.Linear(320, num_classes)\n","        self.classifier2 = nn.Linear(320, num_classes)\n","        self.classifier3 = nn.Linear(320, num_classes)\n","\n","        self.feature_pool = torch.nn.AdaptiveAvgPool2d(output_size=1)\n","        #self.atten_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.crop_resize = AttentionCropLayer()\n","\n","        #l'output delle due apn sono 3 valori, che indicano x,y,l\n","        self.apn1 = nn.Sequential(\n","            nn.Linear(320 * 7 * 7, 4096),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(4096, 3),\n","            nn.Tanh(),\n","        )\n","\n","        self.apn2 = nn.Sequential(\n","            nn.Linear(320 * 7 * 7, 4096),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(4096, 3),\n","            nn.Tanh(),\n","        )\n","        \n","        self.echo = None\n","\n","    def forward(self, x):\n","        #batch_size = x.shape[0]\n","        rescale_tl = torch.tensor([1, 1, 0.5], requires_grad=False).cuda()\n","        # forward @scale-1\n","        feature_s1 = self.b1.features[:-1](x)  # torch.Size([batch_size, 320, 7, 7])\n","        pool_s1 = self.feature_pool(feature_s1) # torch.Size([batch_size, 320, 1, 1])\n","        _attention_s1 = self.apn1(feature_s1.view(-1, 320 * 7 * 7))\n","        attention_s1 = _attention_s1*rescale_tl\n","        resized_s1 = self.crop_resize(x, attention_s1 * x.shape[-1])\n","        # forward @scale-2\n","        feature_s2 = self.b2.features[:-1](resized_s1)  # torch.Size([1, 320, 7, 7])\n","        pool_s2 = self.feature_pool(feature_s2)\n","        _attention_s2 = self.apn2(feature_s2.view(-1, 320 * 7 * 7))\n","        attention_s2 = _attention_s2*rescale_tl\n","        resized_s2 = self.crop_resize(resized_s1, attention_s2 * resized_s1.shape[-1])\n","        # forward @scale-3\n","        feature_s3 = self.b3.features[:-1](resized_s2)\n","        pool_s3 = self.feature_pool(feature_s3)\n","        pred1 = self.classifier1(pool_s1.view(-1, 320))\n","        pred2 = self.classifier2(pool_s2.view(-1, 320))\n","        pred3 = self.classifier3(pool_s3.view(-1, 320))\n","        return [pred1, pred2, pred3], [feature_s1, feature_s2], [attention_s1, attention_s2], [resized_s1, resized_s2]\n","\n","    def __get_weak_loc(self, features):\n","        ret = []   # search regions with the highest response value in conv5\n","        for i in range(len(features)):\n","            resize = 224 if i >= 1 else 448\n","            response_map_batch = F.interpolate(features[i], size=[resize, resize], mode=\"bilinear\").mean(1)  # mean alone channels\n","            ret_batch = []\n","            for response_map in response_map_batch:\n","                argmax_idx = response_map.argmax()\n","                ty = (argmax_idx % resize)\n","                argmax_idx = (argmax_idx - ty)/resize\n","                tx = (argmax_idx % resize)\n","                ret_batch.append([(tx*1.0/resize).clamp(min=0.25, max=0.75), (ty*1.0/resize).clamp(min=0.25, max=0.75), 0.25])  # tl = 0.25, fixed\n","            ret.append(torch.Tensor(ret_batch))\n","        return ret\n","\n","    def __echo_pretrain_apn(self, inputs, optimizer):\n","        inputs = Variable(inputs).cuda()\n","        _, features, attens, _ = self.forward(inputs)\n","        weak_loc = self.__get_weak_loc(features)\n","        optimizer.zero_grad()\n","        weak_loss1 = F.smooth_l1_loss(attens[0], weak_loc[0].cuda())\n","        weak_loss2 = F.smooth_l1_loss(attens[1], weak_loc[1].cuda())\n","        loss = weak_loss1 + weak_loss2\n","        #calcola il gradiente della loss\n","        loss.backward()\n","        #perform a single optimization step\n","        optimizer.step()\n","        #ritorna la loss come un singolo numero anziche un tensore\n","        return loss.item()\n","\n","    @staticmethod\n","    def multitask_loss(logits, targets):\n","        loss = []\n","        criterion = torch.nn.BCEWithLogitsLoss()\n","        criterion = criterion\n","        for i in range(len(logits)):\n","            loss.append(criterion(logits[i], targets))\n","        loss = torch.sum(torch.stack(loss))\n","        return loss\n","\n","    @staticmethod\n","    def rank_loss(logits, targets, margin=0.05):\n","        #as said in the paper\n","        preds = [torch.sigmoid(x) for x in logits] # preds length equal to 3\n","        losses = []\n","        criterion = torch.nn.MarginRankingLoss(margin=0.05)\n","        criterion = criterion\n","        for pred in preds:\n","            loss = []\n","            for i in range(len(pred)-1):\n","                #the loss is the diff between cnn predictions\n","                #rank_loss = (pred[i]-pred[i+1] + margin).clamp(min = 0)\n","                y = Variable(torch.Tensor(pred[0].size(0)).fill_(-1)).cuda()\n","                rank_loss = criterion(pred[i], pred[i+1], y)\n","                loss.append(rank_loss)\n","            loss = torch.sum(torch.stack(loss))\n","            losses.append(loss)\n","        losses = torch.stack(losses)\n","        losses = torch.sum(losses)\n","        return losses\n","    \n","    def __echo_backbone(self, inputs, targets, optimizer):\n","        inputs, targets = Variable(inputs).cuda(), Variable(targets).cuda()\n","        logits, _, _, _ = self.forward(inputs)\n","        optimizer.zero_grad()\n","        # logit --> the vector of raw (non-normalized) predictions that a classification model generates\n","        loss = self.multitask_loss(logits, targets)\n","        loss.backward()\n","        optimizer.step()\n","        return loss.item()\n","\n","    def __echo_apn(self, inputs, targets, optimizer):\n","        inputs, targets = Variable(inputs).cuda(), Variable(targets).cuda()\n","        logits, _, _, _ = self.forward(inputs)\n","        optimizer.zero_grad()\n","        loss = self.rank_loss(logits, targets)\n","        loss.backward()\n","        optimizer.step()\n","        return loss.item()\n","\n","    def mode(self, mode_type):\n","        assert mode_type in ['pretrain_apn', 'apn', 'backbone']\n","        if mode_type == 'pretrain_apn':\n","            self.echo = self.__echo_pretrain_apn\n","            self.eval()\n","        if mode_type == 'backbone':\n","            self.echo = self.__echo_backbone\n","            self.train()\n","        if mode_type == 'apn':\n","            self.echo = self.__echo_apn\n","            self.eval()\n","\n","\n","#if __name__ == \"__main__\":\n","#    net = RACNN(num_classes=6).cuda()\n","#    net.mode('pretrain_apn')\n","#    optimizer = torch.optim.SGD(list(net.apn1.parameters()) + list(net.apn2.parameters()), lr=0.001, momentum=0.9)\n","#    for i in range(50):\n","#        inputs = torch.rand(2, 3, 448, 448)\n","#        print(f':: loss @step{i} : {net.echo(inputs, optimizer)}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"G-GYhrJaNewS"},"outputs":[],"source":["import shutil\n","import cv2\n","import imageio\n","import os\n","import numpy as np\n","import sys\n","import torch\n","import time\n","import torch.optim as optim\n","import torch.backends.cudnn as cudnn\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","\n","sys.path.append('.')  # noqa: E402\n","from torch.autograd import Variable\n","\n","\n","\n","def log(msg):\n","    open('build/core.log', 'a').write(f'[{time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())}]\\t'+msg+'\\n'), print(msg)\n","\n","\n","def random_sample(dataloader):\n","    for batch_idx, (inputs, _) in enumerate(dataloader, 0):\n","        return inputs[0].cuda()\n","\n","\n","def save_img(x, path, annotation=''):\n","    fig = plt.gcf()  # generate outputs\n","    plt.imshow(PlantDataset.tensor_to_img(x[0]), aspect='equal'), plt.axis('off'), fig.set_size_inches(224/100.0/3.0, 224/100.0/3.0)\n","    plt.gca().xaxis.set_major_locator(plt.NullLocator()), plt.gca().yaxis.set_major_locator(plt.NullLocator()), plt.subplots_adjust(top=1, bottom=0, left=0, right=1, hspace=0, wspace=0), plt.margins(0, 0)\n","    plt.text(0, 0, annotation, color='white', size=4, ha=\"left\", va=\"top\", bbox=dict(boxstyle=\"square\", ec='black', fc='black'))\n","    plt.savefig(path, dpi=300, pad_inches=0)    # visualize masked image\n","\n","\n","def run(pretrained_backbone=None):\n","    net = RACNN(num_classes=6).cuda()\n","    if pretrained_backbone:  # Using pretrained backbone for apn pretraining\n","        state_dict = torch.load(pretrained_backbone).state_dict()\n","        net.b1.load_state_dict(state_dict)\n","        net.b2.load_state_dict(state_dict)\n","        net.b3.load_state_dict(state_dict)\n","\n","    cudnn.benchmark = True\n","\n","    params = list(net.apn1.parameters()) + list(net.apn2.parameters())\n","    optimizer = optim.SGD(params, lr=0.001, momentum=0.9)\n","\n","    data_set = get_plant_loader()\n","    trainloader = torch.utils.data.DataLoader(data_set[\"train\"], batch_size=8, shuffle=True)\n","    validationloader = torch.utils.data.DataLoader(data_set[\"validation\"], batch_size=16, shuffle=False)\n","    sample = random_sample(validationloader)\n","    \n","    net.mode(\"pretrain_apn\")\n","\n","    def avg(x): return sum(x)/len(x)\n","    for epoch in range(1):\n","        losses = []\n","        for step, (inputs, _) in enumerate(trainloader, 0):\n","\n","            loss = net.echo(inputs, optimizer)\n","            losses.append(loss)\n","            avg_loss = avg(losses[-5 if len(losses) > 5 else -len(losses):])\n","            print(f':: loss @step{step:2d}: {loss}\\tavg_loss_5: {avg_loss}')\n","\n","            if step % 2 == 0 or step < 5:  # check point\n","                _, _, _, resized = net(sample.unsqueeze(0))\n","                x1, x2 = resized[0].data, resized[1].data\n","                # visualize cropped inputs\n","                save_img(x1, path=f'build/.cache/step_{step}@2x.jpg', annotation=f'loss = {avg_loss:.7f}, step = {step}')\n","                save_img(x2, path=f'build/.cache/step_{step}@4x.jpg', annotation=f'loss = {avg_loss:.7f}, step = {step}')\n","            print(step)\n","            if step >= 128:  # 128 steps is enough for pretraining\n","                torch.save(net.state_dict(), f'build/racnn_pretrained.pt')\n","                return\n","\n","\n","def build_gif(pattern='@2x', gif_name='pretrain_apn_EfficientNet', cache_path='build/.cache'):\n","    # generate a gif, enjoy XD\n","    files = [x for x in os.listdir(cache_path) if pattern in x]\n","    files.sort(key=lambda x: int(x.split('@')[0].split('_')[-1]))\n","    gif_images = [imageio.imread(f'{cache_path}/{img_file}') for img_file in files]\n","    imageio.mimsave(f\"build/{gif_name}{pattern}-{int(time.time())}.gif\", gif_images, fps=8)\n","\n","\n","def clean(path='build/.cache/'):\n","    print(' :: Cleaning cache dir ...')\n","    if os.path.exists(path):\n","        shutil.rmtree(path)\n","    os.makedirs(path)\n","\n","\n","#if __name__ == \"__main__\":\n","    #os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","    #clean()\n","    #run(pretrained_backbone='build/efficientNet_b0_ImageNet.pt')\n","    #build_gif(pattern='@2x', gif_name='pretrain_apn_EfficientNet')\n","    #build_gif(pattern='@4x', gif_name='pretrain_apn_EfficientNet')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"GdXYDa7cN4Vt"},"outputs":[{"name":"stdout","output_type":"stream","text":[" :: Cleaning cache dir ...\n"," :: Start training with build/racnn_pretrained.pt\n"]},{"ename":"RuntimeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-8eafcf8da34b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_VISIBLE_DEVICES\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;31m#RACNN con backbone e apn pre addestrate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'build/racnn_pretrained.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0mbuild_gif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'@2x'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgif_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'racnn_efficientNet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mbuild_gif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'@4x'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgif_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'racnn_efficientNet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-8eafcf8da34b>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(pretrained_model)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf' :: Start training with {pretrained_model}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRACNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \"\"\"\n\u001b[0;32m--> 680\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \"\"\"\n\u001b[0;32m--> 680\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"]}],"source":["import imageio\n","import os\n","import shutil\n","import sys\n","from scipy.sparse import data\n","import torch\n","import time\n","import torch.optim as optim\n","import torch.backends.cudnn as cudnn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","\n","sys.path.append('.')  # noqa: E402\n","\n","\n","\n","def avg(x): return sum(x)/len(x)\n","\n","\n","def train(net, dataloader, optimizer, epoch, _type):\n","    assert _type in ['apn', 'backbone']\n","    losses = 0\n","    net.mode(_type), log(f' :: Switch to {_type}')  # switch loss type\n","    for step, (inputs, targets) in enumerate(dataloader, 0):\n","        loss = net.echo(inputs, targets, optimizer)\n","        losses += loss\n","\n","        if step % 20 == 0 and step != 0:\n","            avg_loss = losses/20\n","            log(f':: loss @step({step:2d}/{len(dataloader)})-epoch{epoch}: {loss:.10f}\\tavg_loss_20: {avg_loss:.10f}')\n","            losses = 0\n","\n","    return avg_loss\n","\n","\n","def test(net, dataloader):\n","    log(' :: Testing on test set ...')\n","    acc = 0\n","    correct_summary = {\n","        'clsf-0': {\n","            'top-1': 0,\n","            },\n","        'clsf-1': {\n","            'top-1': 0,\n","        },\n","        'clsf-2': {\n","            'top-1': 0,\n","            }\n","        }\n","\n","    for step, (inputs, labels) in enumerate(dataloader, 0):\n","        inputs, labels = Variable(inputs).cuda(), Variable(labels).cuda()\n","\n","        with torch.no_grad():\n","            outputs, _, _, _ = net(inputs)\n","            for idx, logits in enumerate(outputs):\n","                logits = torch.sigmoid(logits)\n","                logits[logits >= 0.5 ] = 1\n","                logits[logits < 0.5 ] = 0\n","                correct_summary[f'clsf-{idx}']['top-1'] +=  torch.all(torch.eq(logits, labels),  dim=1).sum()  # top-1\n","                #correct_summary[f'clsf-{idx}']['top-5'] += torch.eq(logits.topk(max((1, 5)), 1, True, True)[1], labels.view(-1, 1)).sum().float().item()  # top-5\n","            if step == 110:\n","                for clsf in correct_summary.keys():\n","                    _summary = correct_summary[clsf]\n","                    for topk in _summary.keys():\n","                        log(f'\\tAccuracy {clsf}@{topk} ({step}/{len(dataloader)}) = {_summary[topk]/((step+1)*int(inputs.shape[0])):.5%}')\n","                        acc +=_summary[topk]/((step+1)*int(inputs.shape[0]))\n","    return acc/3\n","\n","\n","def run(pretrained_model):\n","    accuracy = 0\n","    log(f' :: Start training with {pretrained_model}')\n","    net = RACNN(num_classes=6).cuda()\n","    net.load_state_dict(torch.load(pretrained_model))\n","    cudnn.benchmark = True\n","\n","    #ognuna delle 3 cnn del modello parte con i valori della cnn pre addestrata e poi ognuna si specializza\n","    #con i propri parametri\n","    cls_params = list(net.b1.parameters()) + list(net.b2.parameters()) + list(net.b3.parameters()) + \\\n","        list(net.classifier1.parameters()) + list(net.classifier2.parameters()) + list(net.classifier3.parameters())\n","    apn_params = list(net.apn1.parameters()) + list(net.apn2.parameters())\n","\n","    cls_opt = optim.SGD(cls_params, lr=0.001, momentum=0.9)\n","    #TODO da modificare in lr=1e-6\n","    apn_opt = optim.SGD(apn_params, lr=0.0001, momentum=0.9)\n","\n","    data_set = get_plant_loader()\n","    trainloader = torch.utils.data.DataLoader(data_set[\"train\"], batch_size=16, shuffle=True)\n","    validationloader = torch.utils.data.DataLoader(data_set[\"validation\"], batch_size=16, shuffle=False)\n","    sample = random_sample(validationloader)\n","\n","    for epoch in range(20):\n","        net.train()\n","        cls_loss = train(net, trainloader, cls_opt, epoch, 'backbone')\n","        rank_loss = train(net, trainloader, apn_opt, epoch, 'apn')\n","        net.eval()\n","        temp_accuracy = test(net, validationloader)\n","\n","        # visualize cropped inputs\n","        _, _, _, resized = net(sample.unsqueeze(0))\n","        x1, x2 = resized[0].data, resized[1].data\n","        save_img(x1, path=f'build/.cache/epoch_{epoch}@2x.jpg', annotation=f'cls_loss = {cls_loss:.7f}, rank_loss = {rank_loss:.7f}')\n","        save_img(x2, path=f'build/.cache/epoch_{epoch}@4x.jpg', annotation=f'cls_loss = {cls_loss:.7f}, rank_loss = {rank_loss:.7f}')\n","\n","        # save model per 10 epoches\n","        if temp_accuracy > accuracy:\n","            accuracy = temp_accuracy\n","            stamp = f'e{epoch}{int(time.time())}'\n","            torch.save(net.state_dict(), f'build/racnn_efficientNetB0.pt')\n","            log(f' :: Saved model dict as:\\tbuild/racnn_efficientNetB0.pt')\n","            torch.save(cls_opt.state_dict(), f'build/cls_optimizer.pt')\n","            torch.save(apn_opt.state_dict(), f'build/apn_optimizer.pt')\n","\n","\n","if __name__ == \"__main__\":\n","    clean()\n","    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","    #RACNN con backbone e apn pre addestrate\n","    run(pretrained_model='build/racnn_pretrained.pt')\n","    build_gif(pattern='@2x', gif_name='racnn_efficientNet')\n","    build_gif(pattern='@4x', gif_name='racnn_efficientNet')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Iy0DrxmnXShR"},"outputs":[],"source":["!cp -r /content/build/ /content/drive/MyDrive/build"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPhS4DoMN4DOR5pq1l7ECAW","collapsed_sections":[],"mount_file_id":"18QN7x_NYQtgl21tPfcwvxWbZU2uXSZ6v","name":"RACNN.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.9"}},"nbformat":4,"nbformat_minor":0}
